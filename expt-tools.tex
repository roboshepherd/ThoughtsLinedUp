\chapter{Experimental Tools \& Technologies}
\label{expt-tools}
%%%%%%%%%%%%%%%%%%%%%
\section{General methodological issues}
%\subsection{MRS research roadmap}
%\label{fig:res-roadmap}
%\label{expt-tools:mrs-design}
%Most of the researches on MRS are empirical in nature. In Sec. \ref{bg:mrs} we have discussed various  trends and paradigms of MRS research. Here we have discussed briefly a general research roadmap using real-robotic hardware.  This roadmap illustrates the details of MRS research process by answering a few basic questions.\\
%%%
%%%
%\begin{figure}
%\centering
%\includegraphics[width=13cm,height=20cm,angle=0]
%{./dia-files/mrs-res-roadmap.eps}
%%figure caption is below the figure
%\caption{Stages of MRS research using real-robotic hardware}
%\label{fig:res-roadmap} % Give a unique label
%\end{figure}
%%% 
%\textbf{Firstly, what is the target application domain of this MRS ?} This precisely formulates the target problem to be tackled within this application domain (Fig. \ref{fig:res-roadmap}: step 1). Some MRS researches may try to implement and verify the performance of an algorithm inspired from biological social systems, while others may not. Some MRS may focus to solve real-life problems like emergency search and rescue in a disaster site, while others may concentrate on increasing productivity of a manufacturing shop-floor. The selection of this application domain will most likely determine the tasks to be done by individual or group of robots. This will lead to select suitable MRS paradigm that can provide suitable solutions to the problem, and this selected paradigm will indicate whether we need powerful  robots with rich sensors/processors or relatively incapable cheap robots for doing some particular tasks (Fig. \ref{fig:res-roadmap}: step 2). These tasks and robot group characteristics can be described by using any existing MRS taxonomies, e.g., taxonomies provided by \cite{Gerkey+2004} and \cite{Dudek+1996} (see Sec. \ref{bg:mrs:taxonomies}) .\\ 
%%%
%\textbf{Secondly, what are the working models or overall organizing principles (i.e., control architecture) of the robot group and individual robots ?} From the task requirements and selected robot's capabilities, one need to fit the MRS into a suitable models of control and architecture (Fig. \ref{fig:res-roadmap}: step 3). In Sec. \ref{bg:mrs} we reviewed  most common modelling techniques of MRS, i.e. behaviour-based or probabilistic modelling approaches with three kinds of architectures of control: classic knowledge-based, traditional market-based/role-based and bio-inspired swarm robotic paradigm. Upon selecting a model, most important characteristics of target MRS: e.g., design of individual robot controller, robot-robot and robot-environment communication and coordination patterns etc. will be revealed . This can be achieved by following a top-down or bottom-up design methodology \cite{Crespi+2008}. For example, one can identify the group behaviours first and then devise individual robot controllers from it. Alternately, robot controllers can be designed first then some formal methodologies can be applied to find the group behaviours (Fig. \ref{fig:res-roadmap}: step 4).\\
%%%
%\textbf{Thirdly, what enabling tools and technologies (hardware and software) are required to function the whole robot group autonomously ?} Whatever be the architectural design of a MRS, we need to ensure that whole group can maintain the necessary level of task performance through the desired interaction and communication strategies. For reducing cost and other practical reasons, individuals robots may not have the capabilities to localize itself without the help of an external GPS or camera etc. The enabling hardware/software technologies will make-up this individual robot's short-comings (Fig. \ref{fig:res-roadmap}: step 5). Moreover, based-on a selected communication technology we need to set-up necessary robot-robot and robot-environment inter-networking infrastructure, e.g., network switches, gateways etc. In order to observe and record experiments and its data for further analysis and improvement, researchers may also employ  extra hardware and software logging tools.\\
%%%
%\textbf{Fourthly, what will be the overall software architecture and how the individual components will be implemented ?} This will lead to design or select suitable robot control software architecture and programming language(s) for implementing that architecture (Fig. \ref{fig:res-roadmap}: step 6). Many existing frameworks are available for low-level robot control, e.g. player/stage \footnote{http://playerstage.sourceforge.net/}. However, high-level robot control often requires custom application development that glues the application code to the low-level robot control code following a solid debugging process.\\
%%%
%\textbf{Fifthly, what hardware/software or control parameters are required to be calibrated ?}  Often robotic researchers use 
%certain hardware, software that need to calibrated (Fig. \ref{fig:res-roadmap}: step 7). For example, to use a camera one needs to calibrate camera and lighting conditions. Low-level calibration of robot sensors may also be required. Suitable software scripts  can automate this process and save huge time during real experiments.\\
%%%
%Thus, after setting-up  a working infrastructure one can think of running real-robotic experiments  Initial trials provide sample data that need to verified to find hidden software bugs and run-time errors (Fig. \ref{fig:res-roadmap}: step 8). This can be tedious if proper logging and error-checking system is not employed in code. However, development of source-code control tools make it easier to track changes in software code and improve them (Fig. \ref{fig:res-roadmap}: step 9). Finally, we can get the potential publishable  results after carrying out sufficient iterations of experiments and analysing them properly (Fig. \ref{fig:res-roadmap}: step 10-11).
%%------------------------------------------------------------
\subsection{Real robotic experiments vs. simulations}
Traditionally robotic researchers use software simulation to validate their model before stepping into real-robotic experiments. Simulating a model by software code is easier and much faster compared to real-world experiments. It does not require any sophisticated hardware set-up or time-consuming software debugging. However, in recent times, the abundance of real robot hardware and other necessary tools encourages researchers to test their work in the real systems from the inception of their models. Here we briefly summarize the reasons why we do not follow the traditional ``simulation first'' approach. Instead of comparing both approaches extensively, we present our rationale behind doing all our experiments in real hardware.\\
%%
Firstly, contemporary state-of-the-art agent-based simulation packages are essentially discrete-event simulators that execute models serially in a computer's CPU \cite{Lysenko+2008}. However in real-world systems agents act in parallel and give the robots:  ``what you see is what you act upon'' environment. In simulations that might not be case.\\
%%
Secondly, the robot-robot and robot-environment interactions are complex and completely unpredictable in real environment than their simulation counterparts. Unexpected failures and inter-agent interactions will not occur in simulations that can either cause positive or negative effects in the experimental results \cite{Krieger+2000}.\\
%%
Thirdly, it is not easy to faithfully model communication behaviours of agents in simulations. In our case, we use short-range Bluetooth communication system which is subject to dynamic noise and limited bandwidth conditions. \\
%%
Fourthly, the dynamic environment conditions, e.g., increased physical interferences of larger team of robots, can also influence the experiment's outcome which may not become obvious in simulations.\\
%%
Finally, we believe that the algorithm tested in real-robots can give us strong confidence for implementing them in real-robotic systems and later on, this can also be extended or verified in simulations. However, conversely speaking, algorithm implemented in simulation has no warranty that this might work practically with large number of robots.
%--------------------------------------------------------------
\subsection{Design of multi-robot system}
\label{expt-tools:mrs-design}
\textbf{Task specification.} We intend to design our target MRS for emulating multi-robot manufacturing scenario where robots are required to perform some shop-tasks in different machines while maintaining an effective MRTA. The notion of {\em tasks} has been kept very simple as our robots are not capable of doing many high-level practical tasks e.g. gripping or recognizing objects, carrying loads etc. Many researchers use additional hardware modules with their robots e.g., gripper to collect pucks or any small objects from floors. Rather than emulating such trivial acts, we have  concentrated on the generic and abstract implementation of our algorithm, that can later be fitted to do any real job. Doing real manufacturing tasks has been kept as a future research issue. Thus by ``doing a task'' our robots usually perform two functions: 1) navigate to a fixed task-location in the experiment arena (hereafter called {\em navigation}), and 2) they do so by avoiding any dynamic obstacle {hereafter called {\em obstacle avoidance}}. In MRS literature this is considered as an equivalent of {\em homing} behaviour (e.g. \citeasnoun{Mataric1994}). Depending on the time-out value of doing a task, a robot can wait at task-location if it arrives earlier or may switch to a different task and change direction on-the-fly. These symbolic tasks can be mapped to any suitable real task in multi-robot manufacturing domain, such as, material handling or attending a machine for various production or maintenance jobs e.g., welding different machine parts, cleaning or doing maintenance work of a machine etc.\\
%%
\textbf{Robot-team specification}. Based on the task-requirements we have selected a simple miniature mobile robot, E-puck \cite{Mondada+2009}, that can do the above navigation tasks avoiding any dynamic or static obstacle. According to the classification of \cite{Dudek+1996} our system can be described as below.\\
%%
\begin{itemize}
\item \texttt{\textbf{SIZE-INF:}} The robot team size is larger than 2 robots and the number of potential tasks. Actually we have used  8 to 16 robots in two sets of experiments and  the average number of robots per task has been kept as 4.
\item \texttt{\textbf{COM-INF:}} Robots can communicate with any other robot.
\item \texttt{\textbf{TOP-ADD:}} Every robot can communicate with any other robot by name or address (link path). Naming of robot's communication link path is assumed to follow any simple convention, e.g., /robot1, /robot2 etc.
\item \texttt{\textbf{BAND-INF:}} Every robot can communicate with other robot with as much bandwidth as necessary. Since our robots need to exchange simple messages we ignore this bandwidth issue.
\item \texttt{\textbf{ARR-DYN:}} Robot can change the arrangement dynamically.
\item \texttt{\textbf{CMP-HOM:}} Each robot is initially identical in both hardware and software, but they can become different gradually by learning different tasks by different degrees (in software).
\end{itemize}
%%
\textbf{System set-up and organization}. We have closely followed the swarm robotic principles for controlling the group. Although we do not impose any necessity for local communication and interaction. The details of our control architecture has been described in Sec. \ref{expt-tools:arch}. Since our robots can not localize themselves by their own hardware we provided them their instant position and orientation ({\em pose}) data information from a multi-robot tracking system. Sec. \ref{expt-tools:swistrack} describes about our tracking system. This system also helps us in recording and logging individual robot's task performance and communication patterns. We use Bluetooth communication link, built-in with our E-puck robots, for host PC to robot communication. Robot can also communicate with each-other physically over Bluetooth. However, we do not use that mode of communication. Instead, inter-robot communication is done virtually in host PC's D-Bus IPC channel. This has been illustrated in Sec. \ref{expt-tools:dbus}.
%==============================================================
\section{Hardware}
\subsection{E-puck robots}
%% FIG: Bats navigation
\begin{figure}[htp]
\centering
\subfloat[E-puck robot]{\includegraphics[width=6cm, height=4cm]{snaps/epuck-happy.eps}} 
\hspace{0.25cm}
\subfloat[A binary-coded marker]{\includegraphics[width=4cm, height=4cm]{snaps/20-31412.eps}}
\caption{(a) The E-puck robot with SwisTrack marker on top, (b) A binary coded marker that can be tracked by an overhead camera  using SwisTrack.}
\label{fig:e-puck}
\end{figure}
%%
We use E-puck \footnote{www.e-puck.org} robots developed by Swiss Federal Institute of Technology at Lausanne (EPFL) and now produced by Cyberbotics \footnote{http://www.cyberbotics.com} and some other companies. The upside of using E-puck is: it is equipped with most common sensing hardware, relatively simple in design, low cost, desktop-sized and offered under open hardware/software licensing terms. So any further modification in hardware/software is not limited to any proprietary restriction. However, the downside of using E-puck robot is: it's processor is based on dsPIC micro-controller (lack of standard programming tool-chains), limited amount of memory (lack of on-board camera image processing option) and default communication module is based-on Bluetooth (limited bandwidth and manual link configuration).\\
%%
E-puck can be programmed through C language and this program can be uploaded from PC to robot through wire: $I^{2}C$ and RS232 channel or, through  wireless: Bluetooth communication channel. This can be tedious and time-consuming if one needs to change the robot controller frequently. However, we intend to keep the robot's functionalities very simple and limited to two main tasks: avoiding obstacles and navigating from one place to another. Thus the default hardware of E-puck seems enough for our experiments.\\ 
\begin{table}
\caption{E-puck robot hardware}
\label{table:epuck}
\begin{center}
\begin{tabular}{|l|l|}
\hline \textbf{Feature} & \textbf{Description}\\
\hline Diameter & About 7 cm\\
\hline Motion & Max. 15 cm/s speed (with 2 stepper motors)\\
\hline Battery power & about 3 hours (5Wh LiION rechargeable battery)\\
\hline Processor & 16 bits micro-controller with DSP core,\\ & Microchip dsPIC 30F6014A at 60MHz (about 15 MIPS)\\
\hline Memory & RAM: 8 KB; FLASH: 144 KB \\
\hline IR sensors & 8 IR sensors measuring ambient light and \\ & proximity of obstacles in a range of 4 cm\\
\hline LEDs & 8 red LEDs on a ring and 1 green LED in the body \\
\hline Camera & Colour camera (max. resolution of 640x480) \\
\hline Sound & 3 omni-directional microphones and\\ & on-board speaker capable of playing WAV or tone sounds\\
\hline Communication & Bluetooth wireless (for robot-PC \& robot-robot link)\\
\hline
\end{tabular}
\end{center}
\end{table}
Table \ref{table:epuck} lists the interesting hardware information about an E-puck robot. The 7 cm diameter desktop-sized robot is easy to handle. It's speed and power autonomy is also reasonable compared with similar miniature robots such as Khepera and its peers \cite{Mondada+2009}. The IR sensors provide an excellent capabilities for obstacle avoidance task. We do not make use of the tiny camera of E-puck. The combination of sound and LEDs can be very effective to detect low-battery power or any other interesting event. By default, E-puck is shipped with a basic firmware that is capable of demonstrating a set of it's basic functionalities. Using the supplied Bluetooth serial communication protocol,  {\em BTCom} protocol, it is possible to establish serial communication link between host PC and robot firmware at a maximum possible speed of 115 kbps. From any  text-based modem control and terminal emulation program, e.g. Minicom \footnote{http://alioth.debian.org/projects/minicom/}, one can remotely send BTCom commands to E-puck robot, e.g., set the speed of the motors, turn on/off LEDs and read the sensor values, e.g., read the IR values or capture image of the camera etc.
%------------------------------------
\subsection{Overhead camera}
\begin{table}
\caption{Features of Prosilica GigE Camera GE4900C}
\label{table:ge4900c}
\begin{center}
\begin{tabular}{|l||l|}
\hline \textbf{Feature} & \textbf{Description}\\
\hline Type & CCD Progressive\\
\hline CCD Sensor & 35mm Kodak KAI-16000\\
\hline Size (L x W x H) & 66x66x110 (in mm)\\
\hline Resolution & 16 Megapixels (4872x3248)\\ 
\hline Frame rate & Max. 3 frames per second at full resolution\\
\hline Interface & Gigabit Ethernet (cable length up to 100 meters)\\
\hline Image output & Bayer 8 and 16 bit\\
\hline
\end{tabular}
\end{center}
\end{table}
In order to set-up a multi-robot tracking system, we have selected a state-of-the-art GE4900C colour camera (Fig. \ref{fig:gige-camera}) from Prosilica \footnote{http://www.prosilica.com}. The Prosilica GE-Series camera, are very compact, high-performance machine vision cameras with Gigabit Ethernet interface.  Table \ref{table:ge4900c} lists its main features. This GigE camera is built with \acf{CCD} technology that converts light into electric charge and process it into electronic signals. Unlike in a complementary metal oxide semiconductor sensor, CCD provides a very sophisticated image capturing mechanism that gives high uniformity in image pixels. The 4872x3247 resolution enables us to track a relatively large area e.g., 4m x 3m. In this case, 1 pixel dot in image roughly can represent approximately 1mm x 1mm area. Although the frame rate may seem low initially, but this small frame-rate gives optimum image processing performance with large image sizes, e.g. 16 MB/frame. Prosilica offers both Windows and Linux \acf{SDK} for image capture and other necessary operations. Using this SDK and OpenCV computer vision library \footnote{http://opencv.willowgarage.com/}, we have converted default Bayer8 format image into RGB format image and used that with our tracking software.
%%
\begin{figure}
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=6cm, height=4cm, angle=0]
{./photos/GigE4900C.eps}
\caption{A GigE4900C camera.}
\label{fig:gige-camera} 
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[t]{0.48\linewidth}
\centering
\includegraphics[width=6cm,height=4cm, angle=0]{snaps/bt-usb-hub.eps}
\caption{ A bluetooth hub that attaches multiple Bluetooth USB adapters with Server PC.}
\label{fig:bt-hub} 
\end{minipage}
\end{figure}
%%--------------------------------------------------------------
\subsection{Server PC configuration}
We use Dell  Precision T5400 server-grade PC with the following main technical specifications:
\begin{table}
\caption{Server PC Configuration}
\label{table:server-pc}
\begin{center}
\begin{tabular}{|l||l|}
\hline Processor & Quad-Core Intel Xeon Processor up to 3.33GHz\\ 
& (1333MHz FSB, 64-bit, 2X 6MB L2 cache)\\
\hline RAM & 32GB (4GB ECC DIMMS x 8 slots)\\
\hline Graphics Card & NVIDIA Quadro FX 570 (Memory: 256MB)\\
\hline Hard-disk &  SATA 3.0Gb/s 7200RPM  2 x 250 GB\\
\hline OS & Ubuntu Linux 9.10 64bit\\
\hline
\end{tabular}
\end{center}
\end{table}
This high performance PC has supported us implementing our algorithms without having any fear of running out of resources e.g. CPU or memory.  The maximum supported RAM of a 32 bit PC architecture is limited to 2 GB. But since we have used 32GB RAM in our Server PC, we have selected a 64-bit OS, Ubuntu Linux 9.10 (amd64) \footnote{http://www.ubuntu.com/}.  As an open-source Linux OS,  Ubuntu offers excellent reliability, performance and community support. In order to enable Bluetooth communication in our host PC, we have added 8 USB-Bluetooth adapters (Belkin F8T017) through a suitable USB-Bluetooth hub (Fig. \ref{fig:bt-hub}). 
%%%%%%%%%%%%%%%%%%%%%
\section{Enabling software tools and frameworks}
\subsection{SwisTrack: a multi-robot tracking system}
\label{expt-tools:swistrack}
In almost all types of robotic experiments, vision-based tracking becomes the standard feasible solution for tracking robot positions, orientations and trajectories. This is due to the low cost of camera hardware and availability of plenty of standard image-processing algorithms from computer vision and robotic research community. However, setting-up a real-time multi-agent tracking platform using existing software solutions are not a trivial job. Commercial systems tend to provide sub-millimetre level high precision 3D tracking solutions with a very high price ranging from  40-50 thousands of pounds which is typically greater than the annual budget of a research project! Besides robotics researchers prefer open-source solution to closed-source proprietary one due to the need for improving certain algorithms and applications continuously. Another line of solution can be borrowing certain open-source tracking code from XYZ lab. But it typically ends up with a lot of frustration while tuning parameters manually, fixing the lab lighting conditions, seeing bad performance of programs that frequently leak memory or show segmentation fault and so forth. Graphical user interface (GUI) or camera calibration can hardly be found in those so-called open-source applications. The third option for solving this tracking issue becomes ``re-inventing the wheel'' or hiring some research students to build a system from the scratch. Certainly this is also not feasible due to the limitations in time, resource and skill needed to produce such a solution. Another big issue is the expiration of research fellowship before doing any practical research!\\
%%
From the beginning of our research, we met the above types of scenario. We got very high price quotes from several commercial motion capture solution providers including, Vicon \footnote{http://www.vicon.com} and some others. We tested some well-known and some not-so-well-known open-source object tracking systems including ARTag \footnote{http://www.artag.net}, ARToolKitPlus \footnote{http://studierstube.icg.tu-graz.ac.at/handheld\_ar/artoolkitplus.php}. We also developed our own versions of Open-CV algorithms for tracking colour blobs based on a GNOME application Cynbe's vison-app \footnote{http://muq.org/~cynbe/vision-apps}. However, our algorithms failed to scale well due to the fluctuations in lighting conditions, lack of proper integration of all related components and some other issues \cite{Sarker2008}. Finally, we settled with SwisTrack \cite{Lochmatter+2008} , a state-of-the-art open-source multi-agent tracking platform developed at EPFL, Switzerland . Thanks to the hard working developers and generous sponsors of EPFL who have offered this excellent tool to the scientific research community and empowered many researchers to track multi-agents or  multi-robots out-of-the-box.\\
%% FIG:  expt areana
\begin{figure}
\centering
\includegraphics[width=12cm,height=8cm,angle=0]{./snaps/RILSnapshot1.eps}
\caption{Our experiment arena captured by Prosilica GigE4900C camera mounted on 3m high ceiling.}
\label{fig:expt-arena} % Give a unique label
\end{figure}
%%
%%  FIG swistrack-screenshot
\begin{figure}
\centering
\includegraphics[width=12cm,height=8cm,angle=0]
{./snaps/SwisTrackScreenshot.eps}
\caption{SwisTrack tracking a team of 16 robots under Ubuntu Linux 9.10 OS.}
\label{fig:swistrack-screenshot} 
\end{figure}
%%
With the improved version 4 released in February 2008, SwisTrack is now becoming one of the {\em de-facto} standard tool for multi-robot tracking. Being open-source, flexible, modular and customizable, Swistrack provides a clean development and deployment path for tracking marked or marker-less objects in real-time. SwisTrack is written in C++ using common C/C++ programming libraries and frameworks. The component-based modular development style is very powerful for developing a custom algorithm and wrap it in a custom component. The GUI provides a rich user interface with a clean separation between algorithmic code and parameters used in those algorithms Fig. \ref{fig:swistrack-screenshot}).\\
%%
%%  FIG swistrack-pipeline
\begin{figure}
\centering
\subfloat[Conversion to grayscale]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-gray.eps}} 
\hspace{0.25cm}
\subfloat[Conversion to binary image]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-threshold.eps}}
\vspace{2.5cm}
\subfloat[Blob-detection]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-blob.eps}} 
\hspace{0.25cm}
\subfloat[Bar-code reading]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-barcode.eps}}
\caption{SwisTrack image processing pipeline uses (a) grayscale conversion, (b) threshold the grayscale image, (c) blob detection and (d) circular bar-code reading algorithms.}
\label{fig:swistrack-pipeline}
\end{figure}
%%
In Sec. \ref{expt-tools:dbus} we show that how we append our custom communication components in the image processing pipeline (Fig. \ref{fig:swistrack-pipeline}). This pipeline can be compared with Unix command processing where output of one command becomes the input of another subsequent command and many commands form a chain or pipeline. Here in SwisTrack, at first an image capturing component grab camera image using USB, IEEE1394/Firewire, GigE or other supported interfaces. Then subsequent SwisTrack components work on this image and do various processing e.g., background subtractions, colour conversions, blob-detection, tracking etc. These components follow standard computer-vision algorithms and can be used without any code modification. But if necessary they can also be modified or optimized though changing source code and/or tuning parameters on-the-fly in SwisTrack GUI. As shown in Fig. \ref{fig:swistrack-screenshot}, this GUI can take parameters in real-time and update images. Final output from images, e.g., object position, orientation, trajectory etc. can be sent over standard communication interfaces, e.g. TCP/IP, NMEA etc. SwisTrack has a lot of other features, such as multi-camera tracking, remote-control of SwisTrack over TCP/IP etc. which are documented in SwisTrack Wiki-book documentation \footnote{http://en.wikibooks.org/wiki/Swistrack}.\\
%%
We have set-up SwisTrack with our Prosilica GigE camera GE4900C and configured it for tracking our E-puck robots with their on-top markers (Fig. \ref{fig:e-puck}). These markers are binary-coded numbers (aka {\em circular bar-codes}) that have certain binary bits or chip-lengths. We have used 20 bits bits chip-lengths. In order to uniquely identify the position and orientation of these markers, these binary numbers are encoded with a fixed hamming distance, i.e. differences in bits of any two binary numbers. We have used a fixed hamming distance of 6 bits. As shown in Fig. \ref{fig:e-puck}, these markers are 8cm diameter and clearly identified and tracked by SwisTarck from a camera image resolution of 4872x3248. SwisTrack has no component for grabbing our Prosilica GigE camera and we have developed a Prosilica GigE input component using Prosilica SDK and OpenCV library. The version of SwisTrack that we have used (May 2008 version, SVN no. XX) has worked pretty well except a few minor things, such as real-time configuration changing was very unstable due to our large camera image size (16MB/frame). In order to avoid that we put necessary configurations in SwisTrack project files that is loaded by SwisTrack in the beginning of our experiment runs.\\
%%
Fig. \ref{fig:swistrack-pipeline} shows the components that we have used throughout our experiments. Along with the standard blob detection and circular bar-code reading components, we use our custom D-Bus server communication component that send pose information to D-Bus \acf{IPC} channels (Sec. \ref{expt-tools:dbus}). These components require a little tuning of few parameters, e.g. blob size, blob counts etc. They can be done once and saved in component configuration files for loading them in next runs. Although SwisTrack provides a wide range of components for object trajectory tracking we have not used them yet. we have not saved grabbed camera images as video from within SwisTrack due to heavy CPU load and memory usage. Besides, the video output component of our version of SwisTrack can not produce smooth video files in our set-up. So, we occasionally save image frames in files and most of our experiments, we use Ubuntu Linux's standard desktop  tool, {\em recordmydesktop} \footnote{http://recordmydesktop.sourceforge.net/} for capturing screen as video. The standard fluorescent lightings set-up of our lab has seemed sufficient for our overhead GigE camera and we have prevented interferences of outside sun-lights by putting black blinds in our lab windows. Our camera has been configured automatically through standard configuration files while program start-up.
%---------------------------
\subsection{D-Bus: an inter-process communication protocol}
\label{expt-tools:dbus}
IPC among various desktop software components enable them to talk to each other and exchange data, messages or request of services. Technological advancements in computer and communication systems now allow robotic researchers to set-up and conduct experiments on multi-robot systems (MRS) from desktop PCs. Many compelling reasons, including open licensing model, availability of open-source tools for almost free of cost, community support etc., make Linux as an ideal operating system for MRS research. However the integration of heterogeneous software components in Linux desktop becomes a challenging issue, particularly when each robot-control software needs sensory and other data input from various other software components (e.g. pose data from a pose-tracker, task information from a task-server etc).\\ 
%%
Traditional IPC solutions in a standard Linux desktop, e.g. pipes, sockets, X atoms, shared memory, temporary files etc. (hereafter called {\em traditional IPCs}), are too static and rigid to meet the demand of a dynamic software system \cite{wittenburg2005}. On the other hand, complex and heavy IPC like CORBA\footnote{http://www.omg.org/gettingstarted/corbafaq.htm} fails to integrate into development tool-chains efficiently. They also require a steep learning curve due to their complex implementations. Besides, the failure of Desktop Communication Protocol in system-wide integration and interoperability issues encouraged the development of the D-Bus message bus system, D-Bus for short \cite{Pennington+2010}. This message bus system provides simple mechanisms for applications to talk to one another. In this paper we describe how we exploit the simplicity and power of D-Bus for running a large MRS.\\
%%
Traditional IPCs lack the important requirements of IPC among several heterogeneous software components of a large MRS. Firstly, real-time support in IPC is critical for connecting time-critical control applications. For example, a multi-robot tracking system  can share robot pose information with a \acf{RCC} though  \acf{SHM}. This pose information can be used to help navigating a robot in real-time. However if that multi-robot tracking system crashes and stops writing new pose information into the SHM, RCC has no default mechanism to know that SHM data is outdated. Some form of reference counting mechanism can be used to overcome this issue, but that makes the implementation of RCC complicated and error-prone.\\
%%
Secondly, IPC must be scalable so that adding more software components (thus more robots, sensors, etc.) in the information sharing game do not affect the overall system performance. But clearly this can not be achieved through traditional IPCs, e.g. SHM or temporary files, as the access to computer memory and disk space is costly and time consuming. Thirdly, IPC should be flexible and compatible enough to allow existing software components to join with newly developed components in the information process sharing without much difficulties. Again existing IPCs are too static and rigid to be integrated with multiple software components. Besides, incompatibility often arises among different applications written in different programming languages with different IPC semantics. Fourthly, IPC should be robust, fault-tolerant and loosely coupled so that if one ceases to work others can still continue to work without strange runtime exceptions. Finally, IPC should be implemented simply and efficiently in any modern high level programming languages, e.g., C/C++, Java, Python. Practically, this is very important since IPC will be required in many places of code and application programmers have little time to look inside the detail implementation of any IPC.\\
%%
In this dissertation, we present a scalable and distributed multi-robot control architecture built upon D-Bus IPC that works asynchronously in real-time. It has virtually no limit on the number of software components who share information. By using only the signalling interfaces, SwisTrack \cite{Lochmatter+2008}, an open-source multi-robot tracking tool can be integrated with our multi-robot control framework. All software components are loosely coupled and unlike traditional IPCs, one does not depend on another for setting up and shutting down IPC infrastructure. For example, in case of SHM one software component explicitly needs to set-up and clean-up SHM spaces. In case of D-Bus any software component can join and leave in the information sharing process at any time. Each component implements its own fall-back strategy if desired information from another component is unavailable at any time. Based on a thin C API, D-Bus also provides many binding in common programming languages. In this work, we use {\em dbus-python}, a Python binding for D-Bus, that provide us a very clean and efficient IPC mechanism.
%%
\subsubsection*{D-Bus Overview}
D-BUS was designed from scratch to replace CORBA and Desktop Communication Protocol to fulfil the needs of a modern Linux system. D-BUS can perform basic application IPC as well as it can facilitate sending events, or signals, through the system, allowing different components in the system to communicate. D-BUS is unique from other IPCs in several ways: e.g. 1) the basic unit of IPC in D-BUS is a message, not a byte stream, 2) D-BUS is bus-based and 3) It has separate system-wide and user/session-wide bus \cite{Love2005}. The simplest form of D-Bus communication is process to process. However, it provides a daemon, known as the {\em message bus daemon}, that routes messages between processes on a specific bus. In this fashion, a bus topology is formed (Fig. \ref{fig:dbus-daemon}). Applications can send to or listen for various events on the bus.\\
%%
D-Bus specification \cite{Pennington+2010} provides full details of D-Bus message protocols, message and data types, implementation guidelines etc. Here we discuss some relevant part of this specification. Fig. \ref{fig:dbus-daemon} an example of DBus system structure.
Here a few basic D-Bus terminologies have been introduced from D-Bus literature.\\ 
%%
\begin{figure}
\begin{center}
\includegraphics[width=7cm,height=3.0cm]{./dia-files/dbus-daemon} 
\caption{A typical view of D-Bus message bus system. } 
\label{fig:dbus-daemon}
\end{center}
\end{figure}
%%
\textbf{D-Bus Connection: }
\textit{DBusConnection} is the structure that a program first uses to initiate talking to the D-Bus daemon, Programs can either use DBUS\_BUS\_SYSTEM or DBUS\_BUS\_SESSION to talk to the respective daemons.\\
\textbf{DBus Message: }
It is simply a message between two process. All the DBus intercommunication are done using \textit{DBusMessage}. These messages can have the following four types: method calls, method returns, signals, and errors. The DBusMessage structure can carry data payload, by appending boolean integers, real numbers, string etc. to the message body.\\ 
\textbf{D-Bus Path: }
This is the path of a remote \textit{Object} (capitalized to avoid ambiguity) of target process, e.g. /org/freedesktop/DBus.\\
\textbf{D-Bus Interface: }
This is the interface on a given Object to talk with, e.g. org.freedesktop.DBus.\\
\textbf{D-Bus Method Call: }
This is a type of DBus message that used to invoke a method on a remote Object.\\
\textbf{D-Bus Signal: }
This is a type of DBus message to make a signal emission. Signal messages must have three header fields: PATH giving the object the signal was emitted from, plus INTERFACE and MEMBER giving the fully-qualified name of the signal. Fig. \ref{fig:dbus-signal-protocol} shows the design of robot-status signal that emits over specified interfaces and paths with a data payload of an integer and a string containing robot-status message.\\
\textbf{D-Bus Error: }
This is the structure that holds the error code which occurs by calling a DBus method.
%%
\begin{figure}
\begin{center}
\includegraphics[width=5cm,height=4cm]{./dia-files/dbus-signal-protocol} 
\caption{A typical structure of a D-Bus signal message.} 
\label{fig:dbus-signal-protocol}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%
\subsubsection*{Strategies for Application Integration}
Under D-Bus, there are two basic mechanisms for applications to interact with each other: 1) by calling a remote Object of target application and 2) by emitting a signal for interested applications. To perform a method call on a D-BUS Object, a method call message must be sent to that Object. It will do some processing and return either a method return message or an error message. Signals are different in that they cannot return anything: there is neither a "signal return" message, nor any other type of error message \footnote{http://www.ibm.com/developerworks/linux/library/l-dbus.html}. Thus on D-Bus everything can be done asynchronously without the need of polling.\\ 
%%
D-Bus provides several language bindings for integrating D-Bus to any native application. The core D-BUS API, written in C, is rather low-level and large. Bindings integrate with programming languages and environments, e.g. Glib, Python, Qt and Mono. The bindings provide environment-specific features. For example, the Glib bindings treat D-BUS connections as {\em GObjects} and allow messaging to integrate into the {\em Glib mainloop}. The preferred use of D-BUS is definitely using language and environment-specific bindings, both for ease of use and improved functionality \cite{Love2005}.

%%%%%%%%%%%%%%%%%%%%%
\subsection{BTCom/Myro: E-puck robot control programs}
\label{expt-tools:btcom}
E-puck robot comes with a set of software tools and libraries to program and to monitor low-level sensor and actuator values. The low-level C library with driver code of E-puck robot can be downloaded from E-puck website \footnote{www.e-puck.org}. In order to modify and recompile this library E-puck developers recommend both Windows and Linux cross-compling tool-chains. Under Windows, the MPLAB environment from Microchip \footnote{http://www.microchip.com} can be used with their C30 compiler. We have used this commercial tool for programming E-puck robot (dsPIC micro-controller), since the Linux counterpart, piklab \footnote{http://piklab.sourceforge.net/} has been found unstable and still under-development. A wide variety of boot-loaders, both under Windows and Linux, can be used to upload the {\em .hex} firmware files to E-puck robot over Bluetooth. We have found a most reliable E-puck boot-loader built-in with the trial version of Webots \footnote{http://www.cyberbotics.com/products/webots/} simulator. E-puck website also provides various other tools, e.g. Player robot control framework driver \footnote{http://code.google.com/p/epuck-player-driver/} Matlab interfacing program, E-puck-monitor (under Windows) etc. for monitoring (or setting) sensors (or actuator) values. The default firmware running in E-puck robot is {\em BTCom}. It initializes Epuck robot hardware and waits for user command over Bluetooth serial port. A set of well-defined BTCom commands can be found in Epuck-library documentation. \\
%%
For high-level control of E-puck robot, we have used Myro robot-control framework \footnote{http://wiki.roboteducation.org/} that is developed by Institute for Personal Robots in Education. While BTCom provides a set of user commands for controlling the robot, it does not take care the setting up the Bluetooth serial connection. Moreover the supplied user commands are very primitive in nature. For example, from a high-level perspectives it is more desirable to command a robot for moving at a specific speed for a given time. Under BTCom, a user can not achieve this without interactively giving low-level motor commands, e.g. set left/right motor speed. On the other hand, by setting up Myro framework, the Bluetooth communication with host PC and E-puck robot can easily be set-up under Python's pySerial \footnote{http://pyserial.sourceforge.net/} module. This provides an elegant solution for controlling E-puck from software code. Besides, Myro provides a thin wrapper code for E-puck's BTCom for defining high-level user commands. For example, instead of setting left/right motor speed, a user can send a forward command with speed and time-out as its parameters. With the simplicity and interactivity of Python programming, this wrapper makes E-puck programming and debugging very simple and easy.\\
%% 
The default BTCom has another limitation that it's detection of low battery voltage is almost unnoticeable by naked eye. For running a long time experiment this is critical since we would like to continue our experiments even if a few robots' batteries run out. By the default code of BTCom, a tiny red LED, located near the poer LED in E-puck body, turns on when battery voltage becomes low. This LED light is not visible from a crowd of robots. In order to overcome this issue we modified BTCom so that it can turn of all LEDs when battery voltage becomes critical. This exploited the hardware interrupt signal from Low-Voltage-Detection module of E-puck hardware.\\
%%
Finally we developed our custom navigation and obstacle avoidance algorithms in Python. The navigation function is based on our camera pose information. In each time-step, robot gets it current pose information from our multi-robot tracking system. It then determines its current coordinate (location) relative to the target object and calculates the differences in pose and orientation. To advance forward, it at first corrects its heading based on the difference and then moves forward for a small fixed distance towards the target. Of course, in every time-step, it also checks that if it is located within the target object's boundary and if this is the case, it ceases its motion. Obstacle avoidance algorithm works under the navigation code. While a robot tries to move forward if an obstacle is sensed by its IR sensor it makes a random turn and tries to avoid it. Due to the noisy sensor values, it takes two or a few time-steps to completely get rid of that obstacle.
%%%%%%%%%%%%%%%%%%%%%
\subsection{BlueZ: Linux's Bluetooth communication stack}
\label{expt-tools:bluez}
%\subsection{Bluez}
%\subsection{Link configuration and management}
The physical communication between the host PC and E-puck robot occurs over Bluetooth wireless radio communication channel.  As defined by the Bluetooth Special Interest Group's official technology  info site \footnote{www.bluetooth.com}
 \begin{quote}
{\em ``Bluetooth} technology is a wireless communications technology intended to replace the cables connecting portable and/or fixed devices while maintaining high levels of security. The key features of Bluetooth technology are robustness, low power, and low cost''.
 \end{quote}
The obvious reason for selecting Bluetooth as the communication technology of E-puck is perhaps   due to it's low cost, low battery usage and universality of hardware and software. Each E-puck robot has a Bluetooth radio link to connect to a host PC or nearby other E-puck robots. Under the hood,  this Bluetooth  chip, LMX9820A \footnote{http://www.national.com/opf/LM/LMX9820A.html},   is  interfaced with a Universal Asynchronous Receiver/Transmitter (UART)  microchip of E-puck robot. This Bluetooth chip can be used to access to the UART "transparently" using a bluetooth rfcomm channel. Using this mode, one can access the e-puck as if it is connected to a serial port.  According to the specification of LMX9820A, it supports Bluetooth version 1.1 qualification, This means that the maximum supported data transfer speed is 1Mbit/s. But typically it is configured to use a serial port's 115200 bits/s speed.\\ 
%%
Before starting to use an E-puck robot  one needs to set-up the Bluetooth connection with the robot. Typical Bluetooth connection set-up from a Bluetooth-enabled host PC includes a few manual steps:  detecting the remote Bluetooth device, securely bonding the device (e.g.  exchanging secret keys) and setting-up the target rfcomm or serial connection (over radio) channel. Various Bluetooth software stacks are available under different OSes. Under Linux, BlueZ \footnote{www.bluez.org} becomes the {\em de-facto} standard software platform. The BlueZ stack was initially developed by Max Krasnyansky at Qualcomm \footnote{http://www.qualcomm.com/} and in 2001  they decided to release it under the GPL. The BlueZ kernel modules, libraries and utilities are known to be working prefect on many architectures supported by Linux. It offers full support for Bluetooth device scanning, securely pairing with devices,  rfcomm or serial link configuration, monitoring and so forth. Initial  scanning and secure bonding of E-puck devices can be done by a set of BlueZ tools, namely, {\em hcitool, l2ping, hciconfig, rfcomm} etc. While initializing, BlueZ's  core daemon, {\em bluetoothd}, reads the necessary configuration files (e.g. rfcomm.conf) and dynamically sets up or binds all Bluetooth devices' links and thereafter, routes all low-level communications to them. BlueZ's supplementary package, Hcidump, offers logging raw data of all Bluetooth communications over a host PC's Bluetooth adapter.  In our host PC, we have used USB dongle type  Bluetooth adapter. We have also used various Linux serial connectivity tools, e.g {\em minicom, picocom etc.} to test link configurations and to send BTCom commands to E-puck robots.\\
%%
From the above points, we can see that setting up and maintaining connectivity to E-puck robots through Bluetooth links is  not a trivial task. Thus, one needs to consider automating the process of Bluetooth link set-up and verification in order to save time in initializing real experiments. Moreover, comparing with other common wireless technologies, e.g. Wifi, Bluetooth is a relatively low-bandwidth technology. In case of almost all wireless technologies, presence of lots of wireless devices  causes significant noises and interferences. Thus, one also needs to consider the channel capacity or total available bandwidth for communications. Within the context of our experiments, we have got an interesting open question. {\em What is the maximum number of E-puck robots that can talk to host PC simultaneously ?}. However, finding the answer of this question is beyond the scope of this thesis and here we would only like  to stick with the feasible configuration of Bluetooth links without modifying any low-level protocols or technical implementation.
%%-------------------------------------------------------------------
\subsection{Python's Multiprocessing: process-based multi-threading}
\label{expt-tools:python}
The real-time interactions among multiple software applications often require concurrency and synchronization, to some degrees, in their functions. Although a common IPC protocol, e.g. D-Bus, solves the problem of data-sharing among different application processes, synchronization of data in various processes remains a challenging issue.   The idea of simultaneous and parallel execution of different part of application codes without any IPC, typically on  multiple CPU cores, introduces the notion of {\em multi-threading} programming. Both process-based and thread-based approach of  program execution has pros and cons. Threads are light weight and they can share memory and state with the parent process without dealing with the complexity of IPC.   Threads can be useful to the algorithms which rely on shared data/state. They can increase throughput by processing more information faster. They can also reduce latency and improve the responsiveness of an application, such as GUI actions. However, since threads  implicitly ``share everything'' â€“ programmers have to protect (lock) anything which will be
shared between threads. Thus thread-based programs are subject to face race conditions or deadlocks among multiple threads.\\
%%
On the other hand, processes are independent process-of-control and they are isolated from each other by the OS. In order to do any data/state sharing they must use some form of IPC to communicate and coordinate. Comparing with threads, processes are big and heavy since process creation takes time and these processes also tends to be large in program size and memory footprint.  Since processes ``share nothing'' -- programmers must explicitly
 share any data/state with suitable mechanism. From a high-level robotic programmer's point of view, both thread-based and process-based application design approaches are disadvantageous. Since thread-based approach requires careful attention in data-sharing it becomes very difficult to design bug-free program in short time-scale. On the other hand process-based approach requires to set-up IPC mechanisms and manage them. However, the latter approach is less likely to produce bugs as data sharing is explicit.\\ 
%%
Almost all modern computer OSes and {\em high-level} programming languages, e.g. C/C++, Java etc. offer multi-threading support. However implementation of multi-threading programming involves lots of low-level thread management activities. In this respect {\em very high-level} programming languages e.g. Python, Ruby etc. offer more efficient and elegant solutions for dealing with multi-theaded programs. Along with this multi-threading issue, various other factors influence us to use Python for coding our high level robotic programs. For example, Python programs are  usually optimized through {\em byte-codes} (like Java programs), and then they are {\em interpreted} by various Python interpreters. Unlike dealing with compiling issues in most of the high-level programming languages, Python allows programmers to focus on their  algorithms more quickly and integrate their systems more effectively.  We have found Python's interactive program development process more productive and flexible than non-interactive programming approach found in some other languages.\\ 
%%
Starting from from version 2.6, Python offers an integration of thread-based programming with process-based programming through it Multiprocessing module \footnote{http://docs.python.org/library/multiprocessing.html}. Traditionally, Python offers threads that are real, OS/Kernel level POSIX {\em pthreads}.  But, older Python programs can have only a single thread to be executing within
 the interpreter at once. This restriction is enforced by the
  so-called \acfi{GIL}. This is a lock which must be
 acquired for a thread to enter the interpreter's space.
 This limits only one thread to  be executing within the Python
 interpreter at once.  This is enforced  in order to keep interpreter maintenance easier.
 But this can also be sidestepped if the application is I/O (e.g. file, socket) bound. A threaded application which makes heavy use of sockets, will not see a huge GIL penalty.  After  doing a lot of research on various alternatives of this approach, Python community has offered Multiprocessing as a feasible solution to side-step GIL by the CPU-bound applications that  require seamless data/state sharing.   It follows the threading API closely but uses processes and
 IPC under the hood. It also offers distributed-computing facilities as well, e.g. remote data-sharing and synchronization.  Thus, we have exploited the power and efficiency of Multiprocessing that enables us to make a modular and flexible implementation of multi-robotic software system. Sec \ref{expt-tools:arch} explains some of our implementations of Python Multiprocessing module.\\
 %%
Python Multiprocessing offers various mechanisms for sharing data among processes or, more precisely speaking, among sub-processes. We have used a separate {\em Manager} process that handles all the data storage tasks and event-based process synchronizations.  Managers are responsible for network and process-based sharing of data between processes (and machines).
 The primary manager type is the {\em BaseManager} that is the basic Manager object, and can easily be subclassed to share
 data remotely.  We use this Multiprocessing Manager object that runs a server process in one machine and offers data objects through proxies in parallel to many client  processes over network interfaces.
%%=================================================================
\section{Multi-robot control architecture}
\label{expt-tools:arch}
As defined by \cite{Mataric2007}, a robot control architecture is ``a set of guiding principles and constraints for organizing a robot's control system''. The overall aim of a multi-robot control architecture is to tie various necessary software components that enable a group of robots to work together and to achieve a common goal, such as self-regulated MRTA, by following a set of guiding principles and constraints. Since our  self-regulated MRTA solution closely follows the bio-inspired swarm robotic system's paradigm, we have selected simple E-puck robots.  Under this paradigm, we have chosen distributed self-organized task-allocation approach (Chapter \ref{afm}) that requires each robot to run its own task-allocation algorithm independently for selecting and switching among tasks (recall from Sec. \ref{bg:mrta} and Fig. \ref{fig:mrta-complexities}).  Robots need to gather task related informations to run these  task-allocation algorithm. Moreover, BTCom  commands, that autonomously drive the robots, are also sent from a host PC's robot controller client program. Robots are also need real-time pose data from multi-robot tracking system. All these requirements indicate us that we need a suitable multi-robot control architecture that can effectively tie all these heterogeneous software components together.  But how can we do that? In this Section, we have answered this question by presenting a multi-robot control architecture, \acfi{HEAD}.  As discussed in Sec. \ref{expt-tools:dbus}, this architecture uses D-Bus IPC mechanism for providing real-time, scalable, fault-tolerant and efficient interactions among various software components.
%%
%%%%%%%%%%%%%%%%%%
\subsection{Hybrid event-driven architecture on D-Bus}
As we have discussed in Sec. \ref{bg:mrs:overview} robotic researchers have spent a lot of efforts for finding suitable robot control architecture. Since last few decades, robot control architectures has been evolving from deliberative to reactive and hybrid (combination of deliberative and reactive), behaviour-based and to some other forms. It has been well established that hybrid control can bring together the best aspects of both reactive and deliberative control by combining the real-time low-level device control and high-level deliberative action control. Only reactive (or deliberative) control approach is not enough for enabling robots to do complex tasks in a dynamic environments \cite{Gat1997}.\\
%%
As shown in Fig. \ref{fig:3-layer-arch}(a), hybrid control is usually achieved by a three-layer architecture composed of deliberator, sequencer and controller . {\em Controller} usually works under real-time reactive feedback control loops to do simple tasks by producing primitive robot behaviours, e.g. obstacle avoidance, wall following etc. {\em Deliberator} performs time-consuming computations, e.g. running exponential search or computer vision algorithm processing. In order to achieve specific task goals, the middle component, {\em sequencer}, typically integrates both deliberator and controller maintaining consistent, robust and timely robot behaviours.\\
%%%
\begin{figure}
\centering
\subfloat[Three layers]{\includegraphics[width=2.5cm,height=1.8cm]{./dia-files/three-layer-arch}} 
\hspace{0.25cm}
\subfloat[Abstract model of HEAD]{\includegraphics[width=5.5cm,height=1.8cm]{./dia-files/abstract-arch}}
\caption{(a) Classical three-layer hybrid robot control architecture after \protect\citeasnoun{Gat1997} 
(b) Our abstract multi-robot control architecture adopted from hybrid architecture.}
\label{fig:3-layer-arch}
\end{figure}
%%
\subsubsection*{Three layers of HEAD}
We have organized our multi-robot control architecture, HEAD into three layers as shown in Fig. \ref{fig:3-layer-arch}(b). Although HEAD has been designed by adopting the principles of hybrid architecture it has many distinct features that are absent or overlooked in a classical hybrid architecture. Firstly, with respect to controller layer, HEAD broadly views sensing and control as communication with external entities. Communication as sensing is not new, e.g. it has been reported in multi-agent learning \cite{Mataric1998}. When robots' on-board computing resources are limited communication can effectively make up their required sensing capabilities. On the other hand, low-level device control is also a series of communication act where actuator commands are typically transmitted over a radio or physical link. Thus, all external communication takes place at the {\em communication layer}. Components sitting in this layer either act as sensors that can receive environmental state, task information, self pose data etc. via suitable communication link or do the real-time control of devices by sending actuator commands over a target communication channel. For example, in this study robot controller clients receive pose data from multi-robot tracking system through this communication layer.  Similarly, from a host-PC, robot controller clients send E-puck robots' actuator commands over Bluetooth wireless radio to physical robots.\\
%%
Secondly, the apparent tight coupling with sensors to actuators has been reduced by introducing a \acfi{DEM} layer. DEM layer acts as a short-term storage of sensor data and various events posted by both controller and deliberator components. Task sequencing has been simplified by automated event triggering mechanism. DEM simply creates new event channels and components subscribe to their interested event channels for reading or writing. If one components updates an event, DEM notifies subscribed components about this event. Controller and deliberator components synchronize their tasks based on this event signals. DEM efficiently serves newly arrived data to the controller and deliberator components by this event sharing mechanism. For example, upon receiving robot pose data, our robot controller clients save this data and notifies to other components about the availability of pose data so that they can do their work using this updated pose data. Thus unlike traditional hybrid architecture, neither specialized languages are needed to program a sequencer nor cumbersome if/else checks are present in this layer.\\
%%
Finally, deliberator layer of HEAD has been described as an {\em application layer} that runs real-application code based on high-level user algorithms as well as low-level sensor data and device states. For example, our self-regulated MRTA algorithms have been fitted in this layer. In classic hybrid architecture the role of this layer has been described mainly in two folds: 1) producing task plan and sending it to sequencer and 2) answering queries made by sequencer. Application layer of HEAD follows the former one by generating plan and queuing it to DEM layer, but it does not support the latter one. DEM layer never makes a query to an application since it acts only as a passive information gateway. Thus this reduced coupling between DEM and application layer has enhanced HEAD with additional robustness and scalability. Additional applications can be added with DEM layer's existing or new event interfaces. Any malfunction or failure in application layer or even in communication layer can be isolated without affecting others. 
%%
\begin{figure*}
\begin{center}
\includegraphics[width=12cm,height=8cm]{./dia-files/concrete-arch} 
%\centering
\caption{General outline of {\em HEAD}. A RCC application has been split into two parts: one runs locally in server PC and another runs remotely, e.g., in an embedded PC.} 
\label{fig:concrete-arch}
\end{center}
\end{figure*}
%%-----------------------------------------------------------
\subsection{Software component integrations}
\label{expt-tools:arch:integration}
Fig. \ref{fig:concrete-arch} outlines the placements of various software components based on their functional characteristics and processing requirements. Here we have discussed how these  software components can be integrated in the communication layer of HEAD using D-Bus IPC. The exact implementation of each of these components are left to be discussed in the following chapters within our specific context of MRTA applications. Note that, here we use the term {\em software component} or {\em application} to denote the logical groupings of several sub-processes or threads that works under a mother process or main thread (here we use the term thread and process interchangeably). Software components that follows our three-layer architecture for grouping its processes are called {\em native component} whereas existing software applications are called {\em external component}. As shown in Fig. \ref{fig:concrete-arch}, RCC and task-information provider,  \acfi{TPS} are native software components of HEAD, whereas SwisTrack \cite{Lochmatter+2008}, an external tool used with HEAD, is called an external component.\\
%%
In order to integrate both native and external components with HEAD. We have designed two separate communication process: a D-Bus signal reception process, {\em SignalListener}, and a D-Bus signal emission process, {\em SignalEmitter}. Inside a native component both of this process can communicate with data and event management process, DataManager, by using any suitable mechanisms, such as, multi-threading, multi-processing (offered in Python multiprocessing as discussed in Sec. \ref{expt-tools:python}), TCP or any other networking protocol.\\
%%
Any external component that intend to act as a sensing (actuating) element of HEAD need to implement a SignalEmitter (SignalListener). For example, we extend SwisTrack with D-Bus signal emitting code (aka SignalEmitter) so that it can emit robot pose messages to individual robots D-Bus path under a common interface.  This emitted signal is then caught by SignalListener of individual robot's RCC. Thus the tight-coupling between SwisTrack and RCC has been removed. During run-time SwisTrack can flexibly track variable number of robots and broadcast their corresponding pose messages without any re-compilation of code. Moreover, in worse cases, if SwisTrack or RCC crashes it does not affect any other component at run-time.\\
%%
Expanding SignalEmitter and SignalListener for more D-Bus signals does not require to make any change the IPC implementation code. Steps for setting up a SignalEmitter is listed below.\\
%%
\textbf{Step 1:} Connect to a D-Bus daemon. \\
Sample C code:
\lstset{language=C,basicstyle=\small}
\begin{lstlisting}
DBusError error;
DBusConnection *conn;
dbus_error_init (&error);
/* Get a connection to the session bus */
conn = dbus_bus_get (DBUS_BUS_SESSION, &error);
\end{lstlisting}
\textbf{Step 2:} Optionally, reserve a D-Bus path or service name (this is not required if the same B-Bus path is not used by any other process).\\
\textbf{Step 3:} Send a signal to a specified path.\\
Sample C code:
\begin{lstlisting} 
DBusMessage *message;
message = dbus_message_new_signal("/target/dbus/path",
"target.dbus.interface","SignalData");
/* Send the signal */
dbus_connection_send (connection, message, NULL);
dbus_message_unref (message);
\end{lstlisting}
%%
\textbf{Steps for setting up signal reception:}\\ 
A signal can be received by setting up a suitable event loop under any supported language bindings. This event loop include a special callback function that become activated when this signal received properly. For example, the callback function of a robot-pose signal can save the pose data into memory or files upon receiving it. This event loop often includes another error handling function that become activated when an error is occurred while receiving the signal. Using a Glib {\em mainloop} interface \footnote{http://library.gnome.org/devel/glib/}, the simplified steps of a typical SignalListener is listed below.\\
%%
\textbf{Step 1:} Set-up a Glib event-loop.\\
Sample C Code:
\begin{lstlisting} 
/* glib main loop */
GMainLoop *loop;
loop = g_main_loop_new(NULL,FALSE);
\end{lstlisting} 
%%
\textbf{Step 2:} Connect to a D-Bus daemon (same as above).\\
\textbf{Step 3:} Add a match for the target D-Bus signal\\
Sample C Code:
\begin{lstlisting} 
/* D-bus signal match */
dbus_bus_add_match (connection,
"type='signal',interface='target.dbus.interface'",NULL);
dbus_connection_add_filter (connection, 
 dbus_signal_callback, loop, NULL);
\end{lstlisting} 
%%
\textbf{Step 4:} Set-up DBus-Glib call.\\
Sample C Code:
\begin{lstlisting} 
/* dbus-glib call */
dbus_connection_setup_with_g_main(connection,NUL);
\end{lstlisting} 
%%
\textbf{Step 5:} Run Glib event-loop.\\
Sample C Code:
\begin{lstlisting} 
/* run glib main loop */
g_main_loop_run(loop);
\end{lstlisting} 
%%
In the above listing \texttt{dbus\_signal\_callback} function contains the specific application code that determines what to do with the received signal (which is not shown here). \texttt{ dbus\_connection\_add\_filter} function can take error handing function as its last argument (or can be NULL as shown here). D-Bus signal match rules are specified in D-Bus specification \cite{Pennington+2010}. In both of the above cases, i.e. signal listening and receiving, in order to add more signals we just need to repeat step 3  as many times as we need. A basic implementation of both of signal emission and listening processes in Python language can be found in this here \footnote{http://dbus.freedesktop.org/doc/dbus-python/doc/tutorial.html}. In Chapter \ref{afm} Chapter \ref{local-comm}, we have discussed these Pythonic way of D-Bus signal handing within the context of our MRTA applications.
%%%%%%%%%%%%%%%%%%%%%
%\section{Other software tools and frameworks}
%\subsection{Player/Stage framework}
%\subsection{Webots simulator}
%\subsection{Boost C++ library}
%\subsection{Microchip MPLAB environment}