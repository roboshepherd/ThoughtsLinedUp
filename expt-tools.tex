\chapter{Experimental Tools and Technologies}
\label{expt-tools}
%%%%%%%%%%%%%%%%%%%%%
\section{General methodological issues}
%%------------------------------------------------------------
\subsection{Real robotic experiments vs. simulations}
Traditionally robotic researchers use software simulation to validate their model before stepping into real-robotic experiments. Simulating a model by software code is easier and much faster compared to real-world experiments. It does not require any sophisticated hardware set-up or time-consuming software debugging. However, in recent times, the abundance of real robot hardware and other necessary tools encourages researchers to test their work in the real systems from the inception of their models. The reasons for not follow the traditional ``simulation first'' approach have briefly been discussed below. 

Firstly, contemporary state-of-the-art agent-based simulation packages are essentially discrete-event simulators that execute models serially in a computer's CPU \cite{Lysenko+2008}. However in real-world systems agents act in parallel and give the robots:  ``what you see is what you act upon'' environment. In simulations that might not be case.

Secondly, the robot-robot and robot-environment interactions are complex and completely unpredictable in real environment than their simulation counterparts. Unexpected failures will not occur in simulations that can either cause positive or negative effects in the experimental results \cite{Krieger+2000}.

Thirdly, it is not easy to faithfully model communication behaviours of agents in simulations. Short-range Bluetooth communication system used in this study is subject to dynamic noise and limited bandwidth conditions.

Fourthly, the dynamic environment conditions, e.g., increased physical interferences of larger team of robots, can also influence the experiment's outcome which may not become obvious in simulations.

Finally, the author of this thesis believes that the algorithm tested in real-robots can give strong confidence for implementing them in real-robotic systems and later on, this can also be extended or verified in simulations. However, conversely speaking, algorithm implemented in simulation has no warranty that this might work practically with large number of robots.
%--------------------------------------------------------------
\subsection{Design of multi-robot system}
\label{expt-tools:mrs-design}
\textbf{Task specification.} In this dissertation, the target multi-robot system has been designed for emulating manufacturing shop-floor scenario where robots are required to perform some tasks in different machines while maintaining an effective MRTA. The notion of {\em tasks} has been kept very simple as e-puck robots are not capable of doing many high-level practical tasks e.g. gripping or recognizing objects, carrying loads etc. Many researchers use additional hardware modules with their robots e.g., gripper to collect a puck or any small objects from floors. Rather than emulating such trivial acts, more focus has been given on the generic and abstract implementation of AFM, that can later be fitted to do any real job. Doing real manufacturing tasks has been kept as a future research issue.

By ``doing a task'' e-puck robots usually perform two functions: 1) navigate to a fixed task-location in the experiment arena (hereafter called {\em navigation}), and 2) they do so by avoiding any dynamic obstacle {hereafter called {\em obstacle avoidance}}. In robotics literature this is considered as an equivalent of {\em homing} behaviour (e.g. \citeasnoun{Mataric1994}). Depending on the time-out value of doing a task, a robot can wait at task-location if it arrives earlier or may switch to a different task and change direction on-the-fly. These symbolic tasks can be mapped to any suitable real task in multi-robot manufacturing domain, such as, material handling or attending a machine for various production or maintenance jobs e.g., welding different machine parts, cleaning or doing maintenance work of a machine etc.\\
%%
\textbf{Robot-team specification}. Based on the task-requirements a simple miniature mobile robot, e-puck \cite{Mondada+2009},  has been selected that can do the above navigation tasks avoiding any dynamic or static obstacle. According to the classification of \citeasnoun{Dudek+1996} the proposed system can be described as below.
%%
\begin{itemize}
\item \texttt{\textbf{SIZE-INF:}} The robot team size is larger than 2 robots and the number of potential tasks. Actually   8 to 16 robots are used in two sets of experiments and  the average number of robots per task has been kept as 4.
\item \texttt{\textbf{COM-INF:}} Robots can communicate with any other robot.
\item \texttt{\textbf{TOP-ADD:}} Every robot can communicate with any other robot by name or address (link path). Naming of robot's communication link path is assumed to follow any simple convention, e.g., /robot1, /robot2 etc.
\item \texttt{\textbf{BAND-INF:}} Every robot can communicate with other robot with as much bandwidth as necessary. Since  robots need to exchange simple messages this bandwidth issue is ignored.
\item \texttt{\textbf{ARR-DYN:}} Robot can change the arrangement dynamically.
\item \texttt{\textbf{CMP-HOM:}} Each robot is initially identical in both hardware and software, but they can become different gradually by learning different tasks by different degrees (in software).
\end{itemize}
%%
\textbf{System set-up and organization}. The swarm robotic principles has been chosen for controlling the group without having the necessity for local communication and interaction. The details of control architecture has been described in Sec. \ref{expt-tools:arch}. Since e-puck robots can not localize themselves by their own hardware  their instant position and orientation ({\em pose}) data information have been provided from a multi-robot tracking system. Sec. \ref{expt-tools:swistrack} describes about the tracking system. This system also helps  in recording and logging individual robot's task performance and communication patterns. Bluetooth communication link, built-in with e-puck robots, has been used for host PC to robot communication. Robot can also communicate with each-other physically over Bluetooth. However, that mode of communication is not used. Instead, inter-robot communication is done virtually in host PC's D-Bus IPC channel. This has been illustrated in Sec. \ref{expt-tools:dbus}.
%==============================================================
%%%%%%%%%%%%%%%%%%%%%
\section{Enabling software tools and frameworks}
\subsection{SwisTrack: a multi-robot tracking system}
\label{expt-tools:swistrack}
In almost all types of robotic experiments, vision-based tracking becomes the standard feasible solution for tracking robot positions, orientations and trajectories. This is due to the low cost of camera hardware and availability of plenty of standard image-processing algorithms from computer vision and robotic research community. However, setting-up a real-time multi-agent tracking platform using existing software solutions are not a trivial job. Commercial systems tend to provide sub-millimetre level high precision 3D tracking solutions with a very high price ranging from  40-50 thousands of pounds which is typically greater than the annual budget of a research project! Besides robotics researchers prefer open-source solution to closed-source proprietary one due to the need for improving certain algorithms and applications continuously. 

Another line of solution can be borrowing certain open-source tracking code from \textit{XYZ} lab. But it typically ends up with a lot of frustration while tuning parameters manually, fixing the lab lighting conditions, seeing bad performance of programs that frequently leak memory or show segmentation fault and so forth. Graphical user interface (GUI) or camera calibration can hardly be found in those so-called open-source applications. The third option for solving this tracking issue becomes ``re-inventing the wheel'' or hiring some research students to build a system from the scratch. Certainly this is also not feasible due to the limitations in time, resource and skill needed to produce such a solution. Another big issue is the expiration of research fellowship before doing any practical research!

From the beginning of this research,  the above types of problems  have been found.  Several commercial motion capture solution providers (including, Vicon\footnote{http://www.vicon.com}) offered very high price quotes. Some well-known and some not-so-well-known open-source object tracking systems were tested which including ARTag\footnote{http://www.artag.net}, ARToolKitPlus\footnote{http://studierstube.icg.tu-graz.ac.at/handheld\_ar/artoolkitplus.php}. A custom version of Open-CV algorithms was developed for tracking colour blobs based on a GNOME application Cynbe's vison-app\footnote{http://muq.org/~cynbe/vision-apps}. 
%%  FIG swistrack-screenshot
\begin{sidewaysfigure}
\centering
\includegraphics[width=16cm,height=11cm]
{./snaps/SwisTrackScreenshot.eps}
\caption{SwisTrack tracking a team of 16 robots under Ubuntu Linux 9.10 OS.}
\label{fig:swistrack-screenshot} 
\end{sidewaysfigure}
%%

However, those algorithms failed to scale well due to the fluctuations in lighting conditions, lack of proper integration of all related components and some other issues. Finally, SwisTrack \cite{Lochmatter+2008} has been chosen. This is a state-of-the-art open-source multi-agent tracking platform developed at EPFL, Switzerland. Thanks to the hard working developers and generous sponsors of EPFL who have offered this excellent tool to the scientific research community and empowered many researchers to track multi-agents or  multi-robots out-of-the-box.

With the improved version 4 released in February 2008, SwisTrack is now becoming one of the {\em de-facto} standard tool for multi-robot tracking. Being open-source, flexible, modular and customizable, Swistrack provides a clean development and deployment path for tracking marked or marker-less objects in real-time. SwisTrack is written in C++ using common C/C++ programming libraries and frameworks. The component-based modular development style is very powerful for developing a custom algorithm and wrap it in a custom component. The GUI provides a rich user interface with a clean separation between algorithmic code and parameters used in those algorithms Fig. \ref{fig:swistrack-screenshot}).

In Sec. \ref{expt-tools:dbus} it is shown that how one can append custom communication components in the image processing pipeline (Fig. \ref{fig:swistrack-pipeline}). This pipeline can be compared with Unix command processing where output of one command becomes the input of another subsequent command and many commands form a chain or pipeline. Here in SwisTrack, at first an image capturing component grab camera image using USB, IEEE1394/Firewire, GigE or other supported interfaces. Then subsequent SwisTrack components work on this image and do various processing e.g., background subtractions, colour conversions, blob-detection, tracking etc. These components follow standard computer-vision algorithms and can be used without any code modification. But if necessary they can also be modified or optimized though changing source code and/or tuning parameters on-the-fly in SwisTrack GUI.

As shown in Fig. \ref{fig:swistrack-screenshot}, this GUI can take parameters in real-time and update images. Final output from images, e.g., object position, orientation, trajectory etc. can be sent over standard communication interfaces, e.g. TCP/IP, NMEA etc. SwisTrack has a lot of other features, such as multi-camera tracking, remote-control of SwisTrack over TCP/IP etc. which are documented in SwisTrack Wiki-book documentation.\footnote{http://en.wikibooks.org/wiki/Swistrack}.

%% FIG:  expt areana
\begin{figure}[H]
\centering
\hspace*{1cm}
%\includegraphics[width=10cm,height=7.5cm,angle=0]
\subfloat[]{
\includegraphics[width=8cm,angle=-90]
{./snaps/RILSnapshot1.eps}}
\newline
\centering
\subfloat[]{
\includegraphics[width=8cm]
{./snaps/RILCamcorderSnapshot1.eps}}
\caption{The experiment arena captured by (a) a GigE4900C camera mounted on 3m high ceiling (b) an ordinary camcorder.}
\label{fig:expt-arena} % Give a unique label
\end{figure}
%%
\begin{figure}[H]
\centering
\subfloat[E-puck robot]{\includegraphics[width=4cm, height=4cm]{snaps/epuck-happy.eps}} 
\hspace{0.5cm}
\subfloat[A binary-coded marker]{\includegraphics[width=4cm, height=4cm]{snaps/20-31412.eps}}
\caption{(a) The e-puck robot with SwisTrack marker on top, (b) A binary coded marker that can be tracked by an overhead camera  using SwisTrack.}
\label{fig:e-puck}
\end{figure}
%%
As shown in Fig. \ref{fig:expt-arena}, SwisTrack has been setup with  Prosilica GigE camera GE4900C and configured it for tracking e-puck robots with their on-top markers (Fig. \ref{fig:e-puck}). These markers are binary-coded numbers (aka {\em circular bar-codes}) that have certain binary bits or chip-lengths.  Twenty bits chip-lengths is used. In order to uniquely identify the position and orientation of these markers, these binary numbers are encoded with a fixed hamming distance, i.e. differences in bits of any two binary numbers. A fixed hamming distance of 6 bits is selected. As shown in Fig. \ref{fig:e-puck}, these markers are 8cm diameter and clearly identified and tracked by SwisTarck from a camera image resolution of 4872x3248. 

SwisTrack has no component for grabbing Prosilica GigE camera frames.  So  a Prosilica GigE input component is developed using Prosilica SDK and OpenCV library. The version of SwisTrack (May 2008) has worked pretty well except a few minor things, such as real-time configuration changing was very unstable due to a large camera image size (16MB/frame). In order to avoid that  necessary configurations is put in SwisTrack project files that is loaded by SwisTrack in the beginning of an experiment run.
%%
%%  FIG swistrack-pipeline
\begin{figure}[H]
\centering
\subfloat[Conversion to grayscale]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-gray.eps}} 
\hspace{0.25cm}
\subfloat[Conversion to binary image]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-threshold.eps}}
\vspace{1cm}
\subfloat[Blob-detection]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-blob.eps}} 
\hspace{0.25cm}
\subfloat[Bar-code reading]{\includegraphics[width=6cm, height=4cm]{./snaps/ST-barcode.eps}}
\caption{SwisTrack image processing pipeline uses (a)  conversion to grayscale image, (b) threshold the grayscale image to get binary image, (c) blob detection and (d) circular bar-code reading algorithms.}
\label{fig:swistrack-pipeline}
\end{figure}
%%

Fig. \ref{fig:swistrack-pipeline} shows the components that  have been used throughout AFM experiments. Along with the standard blob detection and circular bar-code reading components,  custom D-Bus server communication component is used that send pose information to D-Bus \acf{IPC} channels (Sec. \ref{expt-tools:dbus}). These components require a little tuning of few parameters, e.g. blob size, blob counts etc. They can be done once and saved in component configuration files for loading them in next runs. 

Although SwisTrack provides a wide range of components for object trajectory tracking jobs, camera images has been saved as video from within SwisTrack due to heavy CPU load and memory usage. Besides, the video output component of the version of SwisTrack can not produce smooth video files in this set-up. So,  occasionally  image frames are saved in files and in most of the experiments, Ubuntu Linux's standard desktop  tool, {\em recordmydesktop}\footnote{http://recordmydesktop.sourceforge.net/} is used for capturing screen as video. The standard fluorescent lightings set-up of the arena has seemed sufficient for the overhead GigE camera and the interferences of outside sun-lights have been prevented by putting black blinds in the laboratory windows. This GigE camera has been configured automatically through standard configuration files while program start-up.
%---------------------------
\subsection{D-Bus: an inter-process communication protocol}
\label{expt-tools:dbus}
IPC among various desktop software components enable them to talk to each other and exchange data, messages or request of services. Technological advancements in computer and communication systems now allow robotic researchers to set-up and conduct experiments on multi-robot systems from desktop PCs. Many compelling reasons, including open licensing model, availability of open-source tools for almost free of cost, community support etc., make Linux as an ideal operating system for robotics research. However the integration of heterogeneous software components in Linux desktop becomes a challenging issue, particularly when each robot-control software needs sensory and other data input from various other software components (e.g. pose data from a pose-tracker, task information from a task-server etc).

Traditional IPC solutions in a standard Linux desktop, e.g. pipes, sockets, X atoms, shared memory, temporary files etc. (hereafter called {\em traditional IPCs}), are too static and rigid to meet the demand of a dynamic software system \cite{wittenburg2005}. On the other hand, complex and heavy IPC like CORBA\footnote{http://www.omg.org/gettingstarted/corbafaq.htm} fails to integrate into development tool-chains efficiently. They also require a steep learning curve due to their complex implementations. 

Besides, the failure of \textit{Desktop Communication Protocol} in system-wide integration and interoperability issues encouraged the development of the D-Bus message bus system, D-Bus for short \cite{Pennington+2010}. This message bus system provides simple mechanisms for applications to talk to one another. In this thesis the simplicity and power of D-Bus have been exploited for running a large MRS.

Traditional IPCs lack the important requirements of IPC among several heterogeneous software components of a large MRS. 
\begin{itemize}
\item Firstly, real-time support in IPC is critical for connecting time-critical control applications. For example, a multi-robot tracking system  can share robot pose information with a \acf{RCC} though  \acf{SHM}. This pose information can be used to help navigating a robot in real-time. However if that multi-robot tracking system crashes and stops writing new pose information into the SHM, RCC has no default mechanism to know that SHM data is outdated. Some form of reference counting mechanism can be used to overcome this issue, but that makes the implementation of RCC complicated and error-prone.
%
\item Secondly, IPC must be scalable so that adding more software components (thus more robots, sensors, etc.) in the information sharing game do not affect the overall system performance. But clearly this can not be achieved through traditional IPCs, e.g. SHM or temporary files, as the access to computer memory and disk space is costly and time consuming.
%
\item Thirdly, IPC should be flexible and compatible enough to allow existing software components to join with newly developed components in the information process sharing without much difficulties. Again existing IPCs are too static and rigid to be integrated with multiple software components. Besides, incompatibility often arises among different applications written in different programming languages with different IPC semantics.
%
\item Fourthly, IPC should be robust, fault-tolerant and loosely coupled so that if one ceases to work others can still continue to work without strange runtime exceptions. 
%
\item Finally, IPC should be implemented simply and efficiently in any modern high level programming languages, e.g., C/C++, Java, Python. Practically, this is very important since IPC will be required in many places of code and application programmers have little time to look inside the detail implementation of any IPC.
\end{itemize}
%
In this dissertation, a scalable and distributed multi-robot control architecture is presented. This is built upon D-Bus IPC and works asynchronously in real-time. By using only the signalling interfaces, SwisTrack can be integrated with a multi-robot control framework. All software components are loosely coupled and unlike traditional IPCs, one does not depend on another for setting up and shutting down IPC infrastructure. For example, in case of SHM one software component explicitly needs to set-up and clean-up SHM spaces. In case of D-Bus any software component can join and leave in the information sharing process at any time. Each component implements its own fall-back strategy if desired information from another component is unavailable at any time. Based on a thin C API, D-Bus also provides many binding in common programming languages. In this work,  {\em dbus-python}, a Python binding for D-Bus,  is used that provides a very clean and efficient IPC mechanism.
%%------
\subsubsection*{D-Bus Overview}
%%
\begin{figure}[H]
\begin{center}
\includegraphics[width=7cm,height=3.0cm]{./dia-files/dbus-daemon} 
\caption{A typical view of D-Bus message bus system. } 
\label{fig:dbus-daemon}
\end{center}
\end{figure}
D-Bus was designed from scratch to replace CORBA and Desktop Communication Protocol to fulfil the needs of a modern Linux system. D-Bus can perform basic application IPC as well as it can facilitate sending events, or signals, through the system, allowing different components in the system to communicate. D-Bus is unique from other IPCs in several ways: e.g. 1) the basic unit of IPC in D-Bus is a message, not a byte stream, 2) D-Bus is bus-based and 3) It has separate system-wide and user/session-wide bus \cite{Love2005}. The simplest form of D-Bus communication is process to process. However, it provides a daemon, known as the {\em message bus daemon}, that routes messages between processes on a specific bus. In this fashion, a bus topology is formed (Fig. \ref{fig:dbus-daemon}). Applications can send to or listen for various events on the bus.

D-Bus specification \cite{Pennington+2010} provides full details of D-Bus message protocols, message and data types, implementation guidelines etc. Here  some relevant parts of this specification have been discussed. Fig. \ref{fig:dbus-daemon} an example of DBus system structure.

Here a few basic D-Bus terminologies have been introduced from D-Bus literature.
\begin{enumerate}
\item \textbf{D-Bus Connection: }
\textit{DBusConnection} is the structure that a program first uses to initiate talking to the D-Bus daemon, Programs can either use\\ DBUS\_BUS\_SYSTEM or DBUS\_BUS\_SESSION to talk to the respective daemons.
\item \textbf{DBus Message: }
It is simply a message between two process. All the DBus intercommunication are done using \textit{DBusMessage}. These messages can have the following four types: method calls, method returns, signals, and errors. The DBusMessage structure can carry data payload, by appending boolean integers, real numbers, string etc. to the message body.
\item \textbf{D-Bus Path: }
This is the path of a remote \textit{Object} (capitalized to avoid ambiguity) of target process, e.g. \textit{/org/freedesktop/DBus}.\\
\item \textbf{D-Bus Interface: }
This is the interface on a given Object to talk with, e.g. org.freedesktop.DBus.
%
\item \textbf{D-Bus Method Call: }
This is a type of DBus message that used to invoke a method on a remote Object.
%
\begin{figure}[H]
\begin{center}
\includegraphics[width=5cm,height=4cm]{./dia-files/dbus-signal-protocol} 
\caption{A typical structure of a D-Bus signal message.} 
\label{fig:dbus-signal-protocol}
\end{center}
\end{figure}
\item \textbf{D-Bus Signal: }
This is a type of DBus message to make a signal emission. Signal messages must have three header fields: D-Bus path and interface and {\em memeber} (exact signal name) giving the fully-qualified name of the signal. Fig. \ref{fig:dbus-signal-protocol}  shows the design of a robot-status signal that can be emitted over a specified interface and path with a data payload of an integer and a string containing the current status of a robot.
%
\item \textbf{D-Bus Error: }
This is the structure that holds the error code which occurs by calling a DBus method.
\end{enumerate}
%%
%%%%%%%%%%%%%%%%%%
\subsubsection*{Strategies for Application Integration}
Under D-Bus, there are two basic mechanisms for applications to interact with each other: 1) by calling a remote Object of target application and 2) by emitting a signal for interested applications. To perform a method call on a D-Bus Object, a method call message must be sent to that Object. It will do some processing and return either a method return message or an error message. Signals are different in that they cannot return anything: there is neither a "signal return" message, nor any other type of error message\footnote{http://www.ibm.com/developerworks/linux/library/l-dbus.html}. Thus on D-Bus everything can be done asynchronously without the need of polling.

D-Bus provides several language bindings for integrating D-Bus to any native application. The core D-Bus API, written in C, is rather low-level and large. Bindings integrate with programming languages and environments, e.g. Glib, Python, Qt and Mono. The bindings provide environment-specific features. For example, the Glib bindings treat D-Bus connections as {\em GObjects} and allow messaging to integrate into the {\em Glib mainloop}. The preferred use of D-Bus is definitely using language and environment-specific bindings, both for ease of use and improved functionality \cite{Love2005}.
%%%%%%%%%%%%%%%%%%%%%
\subsection{BTCom/Myro: e-puck robot control programs}
\label{expt-tools:btcom}
E-puck robot comes with a set of software tools and libraries to program and to monitor low-level sensor and actuator values. The low-level C library with driver code of e-puck robot can be downloaded from e-puck website \footnote{www.e-puck.org}. In order to modify and recompile this library e-puck developers recommend both Windows and Linux cross-compling tool-chains. Under Windows, the MPLAB environment from Microchip\footnote{http://www.microchip.com} can be used with their C30 compiler. This commercial tool is used for programming e-puck robot (dsPIC micro-controller), since the Linux counterpart, piklab\footnote{http://piklab.sourceforge.net/} has been found unstable and still under-development. 

A wide variety of boot-loaders, both under Windows and Linux, can be used to upload the {\em .hex} firmware files to e-puck robot over Bluetooth.  The trial version of Webots\footnote{http://www.cyberbotics.com/products/webots/} simulator provides a most reliable e-puck boot-loader. E-puck website provides various other tools, e.g. Player robot control framework driver\footnote{http://code.google.com/p/epuck-player-driver/} Matlab interfacing program, e-puck-monitor (under Windows) etc. for monitoring (or setting) sensors (or actuator) values. The default firmware running in e-puck robot is {\em BTCom}. It initializes Epuck robot hardware and waits for user command over Bluetooth serial port. A set of well-defined BTCom commands can be found in Epuck-library documentation. 

For high-level control of e-puck robot,  Myro robot-control framework\footnote{http://wiki.roboteducation.org/} is used that is developed by Institute for Personal Robots in Education. While BTCom provides a set of user commands for controlling the robot, it does not take care the setting up the Bluetooth serial connection. Moreover the supplied user commands are very primitive in nature. For example, from a high-level perspectives it is more desirable to command a robot for moving at a specific speed for a given time.

Under BTCom, a user can not achieve this without interactively giving low-level motor commands, e.g. set left/right motor speed. On the other hand, by setting up Myro framework, the Bluetooth communication with host PC and e-puck robot can easily be set-up under Python's pySerial\footnote{http://pyserial.sourceforge.net/} module. This provides an elegant solution for controlling e-puck from software code. Besides, Myro provides a thin wrapper code for e-puck's BTCom for defining high-level user commands. For example, instead of setting left/right motor speed, a user can send a forward command with speed and time-out as its parameters. With the simplicity and interactivity of Python programming, this wrapper makes e-puck programming and debugging very simple and easy.

The default BTCom has another limitation that its detection of low battery voltage is almost unnoticeable by naked eye. For running a long time experiment this is critical since it is desired to continue experiments even if a few robots' batteries run out. By the default code of BTCom, a tiny red LED, located near the power LED in e-puck body, turns on when battery voltage becomes low. This LED light is not visible from a crowd of robots. In order to overcome this issue  BTCom code is modified so that it can turn on all LEDs when battery voltage becomes critical. This exploited the hardware interrupt signal from Low-Voltage-Detection module of e-puck hardware.

Finally, custom navigation and obstacle avoidance algorithms are implemented. The navigation function is based on the camera pose information. In each time-step, robot gets it current pose information from the multi-robot tracking system. It then determines its current coordinate (location) relative to the target object and calculates the differences in pose and orientation. To advance forward, it at first corrects its heading based on the difference and then moves forward for a small fixed distance towards the target. Of course, in every time-step, it also checks that if it is located within the target object's boundary and if this is the case, it ceases its motion. Obstacle avoidance algorithm works under the navigation code. While a robot tries to move forward if an obstacle is sensed by its IR sensor it makes a random turn and tries to avoid it. Due to the noisy sensor values, it takes two or a few time-steps to completely get rid of that obstacle.
%%%%%%%%%%%%%%%%%%%%%
\subsection{BlueZ: Linux's Bluetooth communication stack}
\label{expt-tools:bluez}
%\subsection{Bluez}
%\subsection{Link configuration and management}
The physical communication between the host PC and e-puck robot occurs over Bluetooth wireless radio communication channel.  As defined by the Bluetooth Special Interest Group's official technology  info site\footnote{www.bluetooth.com}
 \begin{quote}
 \ssp
{\em ``Bluetooth} technology is a wireless communications technology intended to replace the cables connecting portable and/or fixed devices while maintaining high levels of security. The key features of Bluetooth technology are robustness, low power, and low cost''.
\end{quote}
\sdp
The obvious reason for selecting Bluetooth as the communication technology of e-puck is perhaps   due to its low cost, low battery usage and universality of hardware and software. Each e-puck robot has a Bluetooth radio link to connect to a host PC or nearby other e-puck robots. Under the hood,  this Bluetooth  chip, LMX9820A\footnote{http://www.national.com/opf/LM/LMX9820A.html},   is  interfaced with a Universal Asynchronous Receiver/Transmitter (UART)  microchip of e-puck robot. This Bluetooth chip can be used to access to the UART "transparently" using a Bluetooth \textit{rfcomm channel}. Using this mode, one can access the e-puck as if it is connected to a serial port.  According to the specification of LMX9820A, it supports Bluetooth version 1.1 qualification, This means that the maximum supported data transfer speed is 1Mbit/s. But typically it is configured to use a serial port's 115200 bits/s speed.

Before starting to use an e-puck robot  one needs to set-up the Bluetooth connection with the robot. Typical Bluetooth connection set-up from a Bluetooth-enabled host PC includes a few manual steps:  detecting the remote Bluetooth device, securely bonding the device (e.g.  exchanging secret keys) and setting-up the target rfcomm or serial connection (over radio) channel. Various Bluetooth software stacks are available under different OSes. Under Linux, BlueZ \footnote{www.bluez.org} becomes the {\em de-facto} standard software platform. The BlueZ stack was initially developed by Max Krasnyansky at Qualcomm\footnote{http://www.qualcomm.com/} and in 2001  they decided to release it under the GPL.

The BlueZ kernel modules, libraries and utilities are known to be working prefect on many architectures supported by Linux. It offers full support for Bluetooth device scanning, securely pairing with devices,  rfcomm or serial link configuration, monitoring and so forth. Initial  scanning and secure bonding of e-puck devices can be done by a set of BlueZ tools, namely, {\em hcitool, l2ping, hciconfig, rfcomm} etc. While initializing, BlueZ's  core daemon, {\em bluetoothd}, reads the necessary configuration files (e.g. rfcomm.conf) and dynamically sets up or binds all Bluetooth devices' links and thereafter, routes all low-level communications to them. BlueZ's supplementary package, Hcidump, offers logging raw data of all Bluetooth communications over a host PC's Bluetooth adapter.  In the host PC,  USB dongle type  Bluetooth adapters are used. Various Linux serial connectivity tools, e.g {\em minicom, picocom etc.} are employed to test link configurations and to send BTCom commands to e-puck robots.

From the above points, it is seen that setting up and maintaining connectivity to e-puck robots through Bluetooth links is  not a trivial task. Thus, one needs to consider automating the process of Bluetooth link set-up and verification in order to save time in initializing real experiments. Moreover, comparing with other common wireless technologies, e.g. Wifi, Bluetooth is a relatively low-bandwidth technology. In case of almost all wireless technologies, presence of lots of wireless devices  causes significant noises and interferences. Thus, one also needs to consider the channel capacity or total available bandwidth for communications. 

Within the context of MRTA experiments, an interesting open question has been arisen. {\em What is the maximum number of e-puck robots that can talk to host PC simultaneously?}. However, finding the answer of this question is beyond the scope of this thesis and here the author of this thesis  has preferred to stick with the feasible configuration of Bluetooth links without modifying any low-level protocols or technical implementation.
%%-------------------------------------------------------------------
\subsection{Python's Multiprocessing: process-based multi-threading}
\label{expt-tools:python}
The real-time interactions among multiple software applications often require concurrency and synchronization, to some degrees, in their functions. Although a common IPC protocol, e.g. D-Bus, solves the problem of data-sharing among different application processes, synchronization of data in various processes remains a challenging issue.   The idea of simultaneous and parallel execution of different part of application codes without any IPC, typically on  multiple CPU cores, introduces the notion of {\em multi-threading} programming. 

Both process-based and thread-based approach of  program execution has pros and cons. Threads are light weight and they can share memory and state with the parent process without dealing with the complexity of IPC.   Threads can be useful to the algorithms which rely on shared data/state. They can increase throughput by processing more information faster. They can also reduce latency and improve the responsiveness of an application, such as GUI actions. However, since threads  implicitly ``share everything'' â€“ programmers have to protect (lock) anything which will be
shared between threads. Thus thread-based programs are subject to face race conditions or deadlocks among multiple threads.

On the other hand, processes are independent process-of-control and they are isolated from each other by the OS. In order to do any data/state sharing they must use some form of IPC to communicate and coordinate. Comparing with threads, processes are big and heavy since process creation takes time and these processes also tends to be large in program size and memory footprint.  Since processes ``share nothing'' -- programmers must explicitly share any data/state with suitable mechanism. From a high-level robotic programmer's point of view, both thread-based and process-based application design approaches are disadvantageous. Since thread-based approach requires careful attention in data-sharing it becomes very difficult to design bug-free program in short time-scale. On the other hand process-based approach requires to set-up IPC mechanisms and manage them. However, the latter approach is less likely to produce bugs as data sharing is explicit.
 
Almost all modern computer OSes and {\em high-level} programming languages, e.g. C/C++, Java etc. offer multi-threading support. However implementation of multi-threading programming involves lots of low-level thread management activities. In this respect {\em very high-level} programming languages e.g. Python, Ruby etc. offer more efficient and elegant solutions for dealing with multi-threaded programs. Along with this multi-threading issue, various other factors influence to use Python for coding high level robotic programs. For example, Python programs are  usually optimized through {\em byte-codes} (like Java programs), and then they are {\em interpreted} by various Python interpreters. Unlike dealing with compiling issues in most of the high-level programming languages, Python allows programmers to focus on their  algorithms more quickly and integrate their systems more effectively. Python's interactive program development process is found to be more productive and flexible than non-interactive programming approaches.

Starting from version 2.6, Python offers an integration of thread-based programming with process-based programming through it Multiprocessing module\footnote{http://docs.python.org/library/multiprocessing.html}. Traditionally, Python offers threads that are real, OS/Kernel level POSIX {\em pthreads}.  But, older Python programs can have only a single thread to be executing within the interpreter at once. This restriction is enforced by the so-called \acfi{GIL}. This is a lock which must be
 acquired for a thread to enter the interpreter's space.
 This limits only one thread to  be executing within the Python interpreter at once.  This is enforced  in order to keep interpreter maintenance easier.
 
But this can also be sidestepped if the application is I/O (e.g. file, socket) bound. A threaded application which makes heavy use of sockets, will not see a huge GIL penalty.  After  doing a lot of research on various alternatives of this approach, Python community has offered Multiprocessing as a feasible solution to side-step GIL by the CPU-bound applications that  require seamless data/state sharing.   It follows the threading API closely but uses processes and IPC under the hood. It also offers distributed-computing facilities as well, e.g. remote data-sharing and synchronization.  Thus, the power and efficiency of Multiprocessing is exploited that enables to make a modular and flexible implementation of multi-robotic software system. Sec \ref{expt-tools:arch} explains some of the implementations of Python Multiprocessing module.
 
Python Multiprocessing offers various mechanisms for sharing data among processes or, more precisely speaking, among sub-processes. A separate {\em Manager} process is used that handles all the data storage tasks and event-based process synchronizations.  Managers are responsible for network and process-based sharing of data between processes (and machines).
 The primary manager type is the {\em BaseManager} that is the basic Manager object, and can easily be subclassed to share
 data remotely.  This Multiprocessing Manager object runs a server process in one machine and offers data objects through proxies in parallel to many client  processes over network interfaces.
%%=================================================================
\section{Multi-robot control architecture}
\label{expt-tools:arch}
Controlling a robot in a well-organized manner involves following a control architecture or a set of guiding principles and constraints. A control architecture organizes the structure  of a robot's control software by defining the way in which sensing, reasoning and actions are represented, organized and interconnected \cite{Bekey2005}. The overall aim of a multi-robot control architecture is to tie various necessary software components that enable a group of robots to work together and to achieve a common goal, such as self-regulated MRTA, by following a set of guiding principles and constraints. 

Since  this self-regulated MRTA solution closely follows the bio-inspired swarm robotic system's paradigm,  simple e-puck robots are selected.  Under this paradigm,  distributed self-organized task-allocation approach\\ (Chapter \ref{afm}) is chosen that requires each robot to run its own task-allocation algorithm independently for selecting and switching among tasks (recall from Sec. \ref{bg:mrta} and Fig. \ref{fig:mrta-complexities}).  Robots need to gather task related informations to run these  task-allocation algorithm. Moreover, BTCom  commands, that autonomously drive the robots, are also sent from a host PC's robot controller client program. Robots also need real-time pose data from multi-robot tracking system. All these requirements indicate that a suitable multi-robot control architecture is required that can effectively tie all these heterogeneous software components together.

In this Section,  this question is answered by presenting a multi-robot control architecture, \acfi{HEAD}.  As discussed in Sec. \ref{expt-tools:dbus}, this architecture uses D-Bus IPC mechanism for providing real-time, scalable, fault-tolerant and efficient interactions among various software components.
%%
%%%%%%%%%%%%%%%%%%
\subsection{Hybrid event-driven architecture on D-Bus}
As discussed in Sec. \ref{bg:mrs:overview}, robotic researchers have spent a lot of efforts for finding suitable robot control architecture. Since last few decades, robot control architectures has been evolving from deliberative to reactive and hybrid (combination of deliberative and reactive), behaviour-based and to some other forms. It has been well established that hybrid control can bring together the best aspects of both reactive and deliberative control by combining the real-time low-level device control and high-level deliberative action control. Only reactive (or deliberative) control approach is not enough for enabling robots to do complex tasks in a dynamic environments \cite{Gat1997}.
\begin{figure}[H]
\centering
\subfloat[Three layers]{\includegraphics[width=2.5cm,height=1.8cm]{./dia-files/three-layer-arch}} 
\hspace{0.25cm}
\subfloat[Abstract model of HEAD]{\includegraphics[width=5.5cm,height=1.8cm]{./dia-files/abstract-arch}}
\caption{(a) Classical three-layer hybrid robot control architecture after \protect\citeasnoun{Gat1997} 
(b) An abstract multi-robot control architecture adopted from hybrid architecture.}
\label{fig:3-layer-arch}
\end{figure}
%%

As shown in Fig. \ref{fig:3-layer-arch}(a), hybrid control is usually achieved by a three-layer architecture composed of deliberator, sequencer and controller. {\em Controller} usually works under real-time reactive feedback control loops to do simple tasks by producing primitive robot behaviours, e.g. obstacle avoidance, wall following etc. {\em Deliberator} performs time-consuming computations, e.g. running exponential search or computer vision algorithm processing. In order to achieve specific task goals, the middle component, {\em sequencer}, typically integrates both deliberator and controller maintaining consistent, robust and timely robot behaviours.
%%%
\begin{sidewaysfigure}
\begin{center}
\includegraphics[height=11cm]{./dia-files/concrete-arch} 
%\centering
\caption{General outline of {\em HEAD}. A RCC application has been split into two parts: one runs locally in server PC and another runs remotely, e.g., in an embedded PC.} 
\label{fig:concrete-arch}
\end{center}
\end{sidewaysfigure}
%%
\subsubsection*{Three layers of HEAD}
In this study, the multi-robot control architecture HEAD is organized into three layers as shown in Fig. \ref{fig:3-layer-arch}(b). Although HEAD has been designed by adopting the principles of hybrid architecture it has many distinct features that are absent or overlooked in a classical hybrid architecture. 

Firstly, with respect to controller layer, HEAD broadly views sensing and control as communication with external entities. Communication as sensing is not new, e.g. it has been reported in multi-agent learning \cite{Mataric1998}. When robots' on-board computing resources are limited communication can effectively make up their required sensing capabilities. On the other hand, low-level device control is also a series of communication act where actuator commands are typically transmitted over a radio or physical link. Thus, all external communication takes place at the {\em communication layer}. Components sitting in this layer either act as sensors that can receive environmental state, task information, self pose data etc. via suitable communication link or do the real-time control of devices by sending actuator commands over a target communication channel. For example, in this study RCCs receive pose data from multi-robot tracking system through this communication layer.  %Similarly, from a host-PC, RCCs send e-puck robots' actuator commands over Bluetooth wireless radio to physical robots.

Secondly, the apparent tight coupling with sensors to actuators has been reduced by introducing a \acfi{DEM} layer. DEM layer acts as a short-term storage of sensor data and various events posted by both controller and deliberator components. Task sequencing has been simplified by automated event triggering mechanism. DEM layer simply creates new event channels and components subscribe to their interested event channels for reading or writing. If one components updates an event, DEM layer notifies subscribed components about this event. Controller and deliberator components synchronize their tasks based on this event signals. DEM layer efficiently serves newly arrived data to the controller and deliberator components by this event sharing mechanism. For example, upon receiving robot pose data, RCCs save this data and notifies to other components about the availability of pose data so that they can do their work using this updated pose data. Thus unlike traditional hybrid architecture, neither specialized languages are needed to program a sequencer nor cumbersome if/else checks are present in this layer.

Lastly, deliberator layer of HEAD has been described as an {\em application layer} that runs real-application code based on high-level user algorithms as well as low-level sensor data and device states. For example, the self-regulated MRTA algorithms have been fitted in this layer. In classic hybrid architecture the role of this layer has been described mainly in two folds: 1) producing task plan and sending it to sequencer and 2) answering queries made by sequencer. Application layer of HEAD follows the former one by generating plan and queuing it to DEM layer, but it does not support the latter one. DEM layer never makes a query to an application since it acts only as a passive information gateway. Thus this reduced coupling between DEM layer and application layer has enhanced HEAD with additional robustness and scalability. Additional applications can be added with DEM layer's existing or new event interfaces. Any malfunction or failure in application layer or even in communication layer can be isolated without affecting others. 
%%-----------------------------------------------------------
\subsection{Software component integrations}
\label{expt-tools:arch:integration}
Fig. \ref{fig:concrete-arch} outlines the placements of various software components based on their functional characteristics and processing requirements. Here the integration of these software components in the communication layer of HEAD is discussed. The exact implementation of each of these components are left to be discussed in the following chapters within the specific context of MRTA applications. Note that, here  the term {\em software component} or {\em application} is used to denote the logical groupings of several sub-processes or threads that works under a mother process or main thread (here the terms \textit{thread} and \textit{process} are used interchangeably).

Software components that follow this three-layer architecture for grouping its processes are called {\em native component} whereas existing software applications are called {\em external component}. As shown in Fig. \ref{fig:concrete-arch}, RCC and task-information provider,  \acfi{TPS} are native software components of HEAD, whereas SwisTrack \cite{Lochmatter+2008}, an external tool used with HEAD, is called an external component.

In order to integrate both native and external components with HEAD. Two separate communication processes are designed, i.e a D-Bus signal reception process, {\em SignalListener}, and a D-Bus signal emission process, {\em SignalEmitter}. Inside a native component both of this process can communicate with data and event management process, DataManager, by using any suitable mechanisms, such as, multi-threading, multi-processing (offered in Python multiprocessing as discussed in Sec. \ref{expt-tools:python}), TCP or any other networking protocol.

Any external component that intend to act as a sensing (actuating) element of HEAD need to implement a SignalEmitter (SignalListener). For example,  SwisTrack is extended with D-Bus signal emitting code (aka SignalEmitter) so that it can emit robot pose messages to individual robots D-Bus path under a common interface.  This emitted signal is then caught by SignalListener of individual robot's RCC. Thus the tight-coupling between SwisTrack and RCC has been removed. During run-time SwisTrack can flexibly track variable number of robots and broadcast their corresponding pose messages without any re-compilation of code. Moreover, in worse cases, if SwisTrack or RCC crashes it does not affect any other component at run-time.
%====================================================
\section{Summary}
In this chapter the major methodological issues of multi-robot research have been discussed. The design of multi-robot system used in this study has been outlined here. The heterogeneous tools and technologies have been employed to set-up the necessary infrastructure for multi-robot experiments. A flexible multi-robot control architecture HEAD has been proposed and illustrated. The integration of various software components has been tackled in this architecture using the state-of-the-art D-Bus interprocess communication technology. The following chapters MRTA experiments have been implemented using this architecture. 