\chapter{Experimental Tools}
%%%%%%%%%%%%%%%%%%%%%
\section{General methodological issues}
\subsection{Design of MRS}
\label{expt-tools:mrs-design}
Before setting up a MRS platform for doing any practical research, one needs to decide the answers of  a few basic questions. 
Firstly, what  is the target application domain of this MRS ? Some MRS researches may try to implement and verify the performance of an algorithm inspired from biological social systems, while others may not. Some MRS may focus to solve real-life problems like emergency search and rescue in a disaster site, while others may concentrate on increasing productivity of a manufacturing shop-floor.  The selection of this application domain will most likely determine the tasks to be done by individual or group of robots. This will lead to select suitable robots for doing that particular tasks. These tasks and robot group characteristics can be described by using any existing MRS taxonomies, e.g.,  taxonomies provided by   \cite{Gerkey+2004} and \cite{Dudek+1996} are widely used for this purpose. 

Secondly,  what are the organizing principles (i.e., control architecture) of  the robot group in question ? From the task requirements and robot capabilities,  one need to fit the MRS into a suitable paradigm of architecture and control. In Sec. \ref{bg:mrs} we reviewed three most common architectural paradigms of MRSs: classic knowledge-based, traditional market-based/role-based and bio-inspired swarm robotic paradigm. Upon selecting a paradigm, most important characteristics of target MRS: e.g., design of individual robot controller, robot-robot and robot-environment communication and coordination patterns etc. will be revealed.

Thirdly, what enabling tools and technologies (hardware and software) are required to function the whole robot group autonomously ? Whatever is the architectural design of a MRS, we need to ensure that whole group can maintain the necessary level of task performance through the desired interaction and communication strategies. For reducing cost and other practical reasons, individuals robots may not have the capabilities to localize itself without the help of an external GPS or camera etc. The enabling technologies will make-up this individual robot's short-comings. Moreover, based-on a selected communication technology we need to set-up necessary robot-robot and robot-environment inter-networking infrastructure, e.g., network switches, gateways etc.

Finally, what extra hardware and software are necessary to observe and record experiments  and its data for further analysis and improvement ? This extra system becomes an essential part of the target MRS. 

We intend to design our target MRS for emulating multi-robot manufacturing scenario where robots do some shop-tasks in different machines. The notion of tasks has been kept very simple as our robots are not  capable of doing many high-level practical tasks e.g. gripping or recognizing objects, carrying  loads etc. Many researchers use additional hardware modules with their robots (e.g., gripper to collect pucks or any small objects from floors). We have not put any effort for emulating that kind of trivial activities, rather we concentrated on the performance of our algorithm, e.g. in task-allocation, from high-level perspectives. Doing real manufacturing tasks has been kept as a future research issue. Thus by ``doing a task'' our robots usually perform two functions: 1) navigate to a fixed task-location in the experiment arena (hereafter called {\em navigation}), and 2) they do so by avoiding any dynamic obstacle {hereafter called {\em obstacle avoidance}}.  Depending on the time-out value of doing a task, a robot can wait at task-location if it arrives earlier or may switch to a different task and change direction on-the-fly. These symbolic tasks can be mapped to any suitable real task in multi-robot manufacturing domain, such as, material handling or attending a machine for various production or maintenance jobs e.g., welding different machine parts, cleaning or doing maintenance work of a machine etc.

Based on the task-requirements we have selected a simple miniature mobile robot, Epuck, that can do the above navigation tasks  avoiding any dynamic or static obstacle. According to the classification of \cite{Dudek+1996} our system can be described as:\\
\texttt{SIZE-INF:} The robot team size is larger than  2 robots and the number of the tasks. Actually we have kept the robot team size as multiple times larger than the number of tasks.\\
\texttt{COM-INF:} Robots can communicate with any other robot.\\
\texttt{TOP-ADD:} Every robot can communicate with any other robot by name or address (link path). Naming of robot's communication link path is assumed to follow any simple convention, e.g., /robot1, /robot2 etc.\\
\texttt{BAND-INF:} Every robot can communicate with other robot with as much bandwidth as necessary. Since our robots need to exchange simple messages we ignore this bandwidth issue.\\
\texttt{ARR-DYN:} Robot can change the arrangement dynamically.\\
\texttt{CMP-HOM:} Each robot is initially identical in both hardware and software, but they can become different gradually by learning different tasks by different degrees (in software).

We have closely followed the swarm robotic principles for controlling the group. Although we do not impose any necessity for local communication and interaction. The details of our control architecture has been described in Sec. {expt-tools:arch}. Since our robots can not localize themselves by their own hardware we  provided them their instant position and orientation (hereafter called {\em pose}) data information from a multi-robot tracking system (MRTS). Sec. \ref{expt-tools:mrts} describes about our tracking system.  This system also helps us in recording and logging  individual robot's task performance and pose information. We use Bluetooth communication technology, built-in with our E-puck robots, for host PC to robot communication. Robot can also communicate with each-other physically over Bluetooth. However, we do not use that mode of communication. Instead, inter-robot communication is done in host PC's virtual inter-process communication channel. This has been illustrated in Sec. \ref{expt-tools:mrts}.
%%
\subsection{Real robotic experiments vs. simulations}
Traditionally robotic researchers use software simulation to validate their model before stepping into real-robot experiments. Simulating a model by software code is easier and much faster compared to real-world experiments. It does not require any sophisticated hardware setup or time-consuming debugging. However, in modern times the abundance of real hardware systems and tools encourage researchers to test their work in real systems from the inception of their models. Here we briefly summarize the reasons why we do not follow the traditional  ``simulation first'' approach. Instead of comparing both approaches extensively, we present our rationale behind doing all our experiments in real hardware.
 
Firstly, contemporary  state-of-the-art agent-based simulation packages are essentially discrete-event simulators that execute models serially in a computer's CPU \cite{Lysenko+2008}. However in real-world systems agents act in parallel and give us the ``what you see is what you act upon''  environment. In simulations that might not be case.

Secondly, the robot-robot and robot-environment interactions are complex and completely unpredictable than their simulations. Unexpected failures and inter-agent interactions will not occur in simulations that can either cause positive or negative effects in experimental results \cite{Krieger+2000}.

Thirdly, it is not easy to faithfully model communication behaviours of agents in simulations. In our case, we use short-range Bluetooth communication system which is subject to dynamic noise and limited bandwidth conditions. 

Fourthly, the dynamic environment conditions, e.g., increased physical interferences of larger team of robots, can also influence the experiment's outcome which is obvious in simulations.

Finally, we believe that the algorithm tested in real-robots can give us strong confidence for implementing in real-systems and later on, this can also be extended or verified in simulations. However, conversely speaking, algorithm implemented in simulation has no warranty that this will work practically with robots.
%\subsection{Phases of Experimental infrastructure set-up}
%\subsection{Issues in  Multi-robot Tracking and Localization}
%%%%%%%%%%%%%%%%%%%%%
\section{Hardware}
\subsection{E-puck robots}
We use E-puck \footnote{www.e-puck.org} robots developed by Swiss Federal Institute of Technology at Lausanne (EPFL) and now produced by Cyberbotics \footnote{http://www.cyberbotics.com} and some other companies. The upside of using E-puck is: it is equipped with most common sensing hardware, relatively simple in design, low cost, desktop-sized and offered under open hardware/software licensing terms. So any further modification in hardware/software is not limited to any proprietary restriction. However, the  downside of using E-puck robot  is:  it's  processor is based on dsPIC micro-controller (lack of standard programming tool-chains), limited amount of memory (lack of on-board camera image processing option) and default  communication module is based-on Bluetooth (limited bandwidth and manual link configuration). So the programming of E-puck can be done through C language and uploaded from PC to robot through wire: $I^{2}C$ and RS232 channel or, through Bluetooth wireless communication channel. This can be tedious and time-consuming if  one needs to change the robot controller frequently. However, we intend to keep the robot's functionalities very simple and limited to two main tasks: avoiding obstacles and navigating from one place to another. Thus the default hardware of E-puck seems enough for our experiments. 

\begin{table}
\caption{E-puck robot hardware}
\label{table:epuck}
\begin{center}
\begin{tabular}{|l||l|}
\hline \textbf{Feature} & \textbf{Description}\\
\hline Diameter & about 7 cm\\
\hline Motion & max. 15 cm/s speed (2 stepper motors)\\
\hline Battery power & about 3 hours  (5Wh LiION rechargeable battery)\\
\hline Processor & 16 bits micro-controller with DSP core,\\ & Microchip dsPIC 30F6014A at 60MHz (about 15 MIPS)\\
\hline Memory & RAM: 8 KB; FLASH: 144 KB \\
\hline IR sensors & 8 IR sensors measuring ambient light and \\ &  proximity of obstacles in a range of 4 cm\\
\hline LEDs & 8 red LEDs on a ring and 1 green LED in the body \\
\hline Camera & colour camera (max. resolution of 640x480) \\
\hline Sound & 3 omni-directional microphones and\\  & on-board speaker capable of playing WAV or tone sounds\\
\hline Bluetooth & for robot-computer and robot-robot wireless communication\\
\hline
\end{tabular}
\end{center}
\end{table}

Table \ref{table:epuck} lists the interesting hardware information about an E-puck robot. The 7 cm diameter desktop-sized robot is easy to handle. It's speed and power autonomy is also reasonable compared with similar miniature robots such as Khepera and its peers \cite{Mondada+2009}.  The IR sensors provide an excellent capabilities for obstacle avoidance task. Although we do not make use of the tiny camera of E-puck, the combination of sound and LEDs can be very effective to detect low-battery power or any other interesting event. By default, E-puck is shipped with a basic firmware that is capable of demonstrating a set of it's basic functionalities. Using the supplied Bluetooth serial communication protocol (hereafter {\em BTCom protocol}), it is possible to establish serial communication link between host PC and robot firmware at a maximum possible speed of 115 kbps.  Using this protocol, one can remotely send command to  robot, e.g., set the speed of the motors, turn on/off LEDs and read the sensor values, e.g., read the IR values or capture image of the camera etc.

\subsection{Overhead GigE camera}
In order to set-up a multi-robot tracking system (MRTS), we have selected a state-of-the-art GE4900C colour camera from Prosilica \footnote{http://www.prosilica.com}. The Prosilica GE-Series camera,  are very compact, high-performance machine vision cameras with Gigabit Ethernet interface. This has following main features:

\begin{table}
\caption{Features of Prosilica GigE Camera GE4900C}
\label{table:ge4900c}
\begin{center}
\begin{tabular}{|l||l|}
\hline \textbf{Feature} & \textbf{Description}\\
\hline Type & Charge-coupled device (CCD) Progressive\\
\hline CCD Sensor & Kodak KAI-16000\\
\hline Size (L x W x H) & 66x66x110  (in mm)\\
\hline Resolution & 16 Megapixels (4872x3248)\\ 
\hline Frame rate  & Max. 3 frames per second  at full resolution\\
\hline Interface & Gigabit Ethernet (cable length up to 100 meters)\\
\hline Image output & Bayer 8/12 bit\\
\hline
\end{tabular}
\end{center}
\end{table}

CCD technology convert light into electric charge and process it into electronic signals. Unlike in a complementary metal oxide semiconductor (CMOS) sensor,  CCD provides a very sophisticated image capturing mechanism that gives high uniformity in image pixels. The high resolution enables us to track a relatively large area e.g., 4m X 3m. In this case, 1 pixel dot in image roughly can represent approximately 1mm X 1mm area. Although the frame rate may seem low initially,  but this small frame-rate gives optimum image processing performance with large image sizes, e.g. 16 MB/frame. Prosilica offers both Windows and Linux SDK for image capture and other necessary operations. Using this SDK, we have converted default Bayer8 format image into RGB format  image and used that in our tracking software.
%%
\subsection{Server PC configuration}
We use Dell  Precision T5400 server-grade PC with the following main technical specifications:
\begin{table}
\caption{Server PC Configuration}
\label{table:server-pc}
\begin{center}
\begin{tabular}{|l||l|}
\hline Processor & Quad-Core Intel Xeon Processor up to 3.33GHz\\ 
& (1333MHz FSB, 64-bit, 2X 6MB L2 cache)\\
\hline RAM & 32GB (4GB ECC DIMMS x 8 slots)\\
\hline Graphics Card & NVIDIA Quadro FX 570 (Memory: 256MB)\\
\hline Hard-disk &  SATA 3.0Gb/s 7200RPM  2 x 250 GB\\
\hline OS & Ubuntu Linux 9.10 64bit\\
\hline
\end{tabular}
\end{center}
\end{table}
This high performance PC has supported us implementing our algorithms without having any fear of running out of RAM.  Since the maximum supported RAM of a 32 bit PC architecture is limited to 2 GB we select Ubuntu Linux 9.10 64bit OS.  As an open-source OS,  Ubuntu offers excellent reliability, performance and community support. In order to enable Bluetooth communication in our host PC, we have added 8 USB-Bluetooth adapters (Belkin F8T017) through a suitable USB-Bluetooth hub. 
%%%%%%%%%%%%%%%%%%%%%
\section{Enabling software tools and frameworks}
\subsection{SwisTrack: a multi-robot tracking system}
\label{expt-tools:mrts}
In almost all types of robotic experiments, vision-based tracking becomes the standard feasible solution for tracking robot positions, orientations and trajectories. This is due to the low cost of camera hardware and availability of plenty of standard image-processing algorithms from computer vision and robotic research community. However,  setting-up a real-time multi-agent tracking platform using existing software solutions are not a trivial job. Commercial systems tend to provide sub-millimetre level high precision 3D tracking solutions with a very high price ranging from tens of thousands of pounds which is typically greater than the annual research budget of a medium sized research lab in UK! Besides robotics researchers prefer open-source solution to closed-source proprietary one due to the need  for improving certain algorithms and applications continuously. Another line of solution, namely borrowing certain open-source tracking code from XYZ lab, typically ends up with a lot of frustration while tuning parameters manually, fixing the lab lighting conditions, seeing bad performance of programs that frequently leak  memory or show segmentation fault and so forth. GUI or camera calibration can hardly be found in those so-called open-source applications. The third option for solving this tracking issue becomes ``re-inventing the wheel'' or hiring some research students to build a system from scratch. Certainly this is also not feasible due to the limited time, resource and skill in this field. Another big issue is the expiration of research fellowship before doing any practical research!

In the beginning of our research we met the all three types of scenario stated above. We got very high price quotes from several commercial motion capture solution providers including,  Vicon \footnote{http://www.vicon.com} and some others. We tested some well-known and some not-so-well-known open-source object tracking systems including ARTag \footnote{http://www.artag.net}, ARToolKitPlus \footnote{http://studierstube.icg.tu-graz.ac.at/handheld\_ar/artoolkitplus.php}. We also developed our own versions of Open-CV \footnote{http://opencv.willowgarage.com}  algorithms for tracking colour blobs based on a GNOME application Cynbe's vison-app \footnote{http://muq.org/~cynbe/vision-apps}. However, our algorithms failed to scale well due to the fluctuations in lighting conditions, lack of proper integration of all related components  and some other issues \cite{Sarker2008}. Finally, we settled with SwisTrack \cite{Lochmatter+2008} , a state-of-the-art open-source  multi-agent tracking platform developed at  Ecole Polytechnique Federale de Lausanne (EPFL), Switzerland . Thanks to the hard working developers and generous sponsors of EPFL who offer this excellent tool to scientific research community and empower many research labs to track multi-agent systems out-of-the-box.

With the improved version 4 released in February 2008, SwisTrack is now becoming the {\em de-facto} standard tool for multi-robot tracking. Being open-source, flexible, modular and customizable, Swistrack provides a clean development and deployment path for tracking marked or marker-less objects in real-time.  SwisTrack is written in C++ using common C++ libraries and frameworks. The component-based modular development style is very powerful for developing a custom algorithm and wrap it in a custom component. In Sec. \ref{expt-tools:dbus} we show that how we append our custom communication components in the image  processing pipeline.  This pipeline can be compared with Unix command processing where output of one command becomes the input of another subsequent command and many commands form a chain or pipeline. Here in SwisTrack, at first an image capturing component grab camera image using USB, IEEE1394/Firewire, GigE or other supported interfaces (see Fig. \ref{fig:swistrack-pipeline}).  Then subsequent SwisTrack components work on this image and do various processings e.g., background subtractions, colour conversions, blob-detection, tracking etc. These components follow standard computer-vision algorithms and can be used without any code modification. But if necessary they can also be modified or optimized though changing source code and/or tuning parameters on-the-fly. As shown in Fig. \ref{fig:swistrack-screen} SwisTrack GUI can take parameters in real-time and update images. Final output from images, e.g., object position, orientation, trajectory etc. can be sent over standard communication interfaces, e.g. TCP/IP, NMEA   etc. SwisTrack has a lot of other features, such as multi-camera tracking, remote-control of SwisTrack over TCP/IP etc. which are documented in SwisTrack Wiki-book documentation \footnote{http://en.wikibooks.org/wiki/Swistrack}.

We have set-up SwisTrack with our Prosilica GigE camera GE4900C and configured it for tracking about 40 E-puck robots with their on-top markers . These markers are binary-coded numbers (aka {em circular bar-codes}) that have certain binary bits or chip-lengths. We have used 20 bits and 15 bits chip-lengths. In order to uniquely identify the position and orientation of  these markers these numbers are encoded with a fixed hamming distance, i.e. differences in bits of any two numbers. We have used a fixed hamming distance of 6 bits. As seen in Fig. \ref{fig:swistrack-markers} these markers are 8cm diameter and clearly identified and tracked by SwisTarck from a camera image resolution of 4872x3248. SwisTrack has no component for grabbing our Prosilica GigE camera and we have developed  a Prosilica GigE  input component using Prosilica SDK and Open-CV library. The version of SwisTrack that we have used (late May 2008 version, SVN no.) has worked pretty well except a few minor things, such as real-time configuration changing was very unstable due to our large camera image size (16MB/frame).  In order to avoid that we use static configuration files that is loaded by SwisTrack in the beginning of our experiment runs.

Fig. \ref{fig:swistrack-pipeline} shows the components that we have used throughout our experiments. Along with the standard blob detection and   circular bar-code reading components, we use our custom D-Bus server communication component that send pose information to D-Bus IPC channels (see Sec.  \ref{expt-tools:dbus}). These components require a little tuning of few parameters, e.g. blob size, blob counts etc. They can be done once and saved in component configuration files for loading them in next runs. Although SwisTrack provides a wide range of components for object trajectory tracking we have not used them yet. we have not saved grabbed camera images as video  from within SwisTrack due to heavy CPU load and memory usage.  Besides, the video output component of  our version of SwisTrack can not produce smooth video files in our set-up.  So, we occasionally save image frames in files and most of our experiments, we use Ubuntu Linux's standard desktop tool, {\em recordmydesktop} \footnote{http://recordmydesktop.sourceforge.net/}  for capturing screen as video.  The standard fluorescent lightings of our lab seem sufficient for our overhead GigE camera and we have prevented interferences of outside sun-lights by putting black blinds  in the window. Our camera has been configured automatically through standard configuration files while program start-up. 
\subsection{D-Bus: an inter-process communication protocol}
\label{expt-tools:dbus}
Inter-process communications (IPC) among various desktop software components enable them to talk to each other and exchange data, messages or request of services. Technological advancements in computer and communication systems now allow robotic researchers to set-up and conduct experiments on multi-robot systems (MRS) from desktop PCs. Many compelling reasons, including open licensing model, availability of open-source tools for almost free of cost, community support etc., make Linux as an ideal operating system for MRS research. However the integration of heterogeneous software components in Linux desktop becomes a challenging issue, particularly when each robot-control software needs sensory and other data input from various other software components (e.g. pose data from a tracker server, task information from a task server etc).\\ 
Traditional IPC solutions in a standard Linux desktop, e.g. pipes, sockets, X atoms, shared memory, temporary files etc. (hereafter called {\em traditional IPCs}), are too static and rigid to meet the demand of a dynamic software system (\cite{wittenburg2005}). On the other hand, complex and heavy IPC like CORBA fails to integrate into a development tool-chain efficiently. They also require a steep learning curve due to their complex implementations. Besides, the failure of Desktop Communication Protocol (DCOP) in system-wide integration and interoperability issues encouraged the development of the D-Bus message bus system, D-Bus for short (\cite{Pennington+2010}). This message bus system provides simple mechanisms for applications to talk to one another (see details in Section 2). In this paper we describe  how we exploit the simplicity and power of D-Bus  for running a large MRS.\\
In pursuing a suitable multi-robot control architecture for our large number of robots, we have found that traditional IPCs are inadequate to support the important requirements of IPC among several heterogeneous software components of a large MRS. Firstly, real-time support in IPC is critical for connecting time-critical control applications. For example, a multi-robot tracking system (MRTS) can share robot pose information with a robot-controller client (RCC) though shared memory (SHM). This pose information can be used to help navigating a robot in real-time. However if MRTS crashes and stops writing new pose information into the SHM, RCC has no default mechanism to know that SHM data is outdated. Some form of reference counting mechanism can be used to overcome this issue, but that makes the implementation of RCC complicated and error-prone.\\
Secondly, IPC must be scalable so that adding more software components (thus more robots, sensors, etc.) in the information sharing game does not affect the overall system performance. But clearly this can not be achieved through traditional IPCs, e.g. SHM or temporary files,  as the access to computer memory and disk space is costly and time consuming. Thirdly, IPC should be flexible and compatible enough to allow existing software components to join with newly developed components in the information sharing without much difficulties. Again existing IPC mechanisms are too static and rigid to be integrated with existing software components. Besides, incompatibility often arises among different applications written in different programming languages with different semantics of IPC. Fourthly, IPC should be robust, fault-tolerant and loosely coupled so that if one ceases to work others can still continue to work without strange runtime exceptions. Finally, IPC should be implemented simply and efficiently in any modern high level programming languages, e.g., C/C++, Java, Python etc. Practically this is very important since IPC will be required in many places of code and application programmers have little time to look inside the detail implementation of any IPC.\\
%%
Here we present a scalable and distributed multi-robot control architecture built upon D-Bus IPC. D-Bus IPC works asynchronously in real-time. It has virtually no limit how many software components participate in information sharing. In fact, to the best of our knowledge, the performance of D-Bus daemon does not vary if the number of participating software components varies. In this paper we have shown that by using only the signalling interfaces, SwisTrack (\cite{Lochmatter+2008}), an open-source multi-robot tracking tool can be integrated with our multi-robot control framework. All software components are loosely coupled and unlike traditional IPCs, one does not depend on another for setting up and shutting down IPC infrastructure. For example, in case of SHM one software component explicitly needs to set-up and clean-up SHM spaces. In case of D-Bus any software component can join and leave in the information sharing process at any time. Each component implements its own fall-back strategy if desired information from another component is unavailable at any time. Based on a thin C API, D-Bus also provides many binding in common programming languages. In this work, we use {\em dbus-python}, a Python binding for D-Bus, that provide us a very clean and efficient IPC mechanism.\\
\subsection*{D-Bus Overview}
D-BUS was designed from scratch to replace CORBA and DCOP  to fulfil the needs of a modern Linux system. D-BUS can perform basic application IPC as well as it can facilitate sending events, or signals, through the system, allowing different components in the system to communicate. D-BUS is unique from other IPC mechanisms in several ways, e.g. 1) the basic unit of IPC in D-BUS is a message, not a byte stream, 2) D-BUS is bus-based and 3) It has separate system-wide and user/session-wide bus (\cite{Love2005}) . The simplest form of D-Bus communication is process to process. However, it provides a daemon, known as the message bus daemon, that routes messages between processes on a specific bus. In this fashion, a bus topology is formed (see Fig. \ref{fig:dbus-daemon}), allowing processes to speak to one or more applications at the same time. Applications can send to or listen for various events on the bus.\\
D-Bus specification (\cite{Pennington+2010}) provides full details of D-Bus message protocols, message and data types, implementation guidelines and so forth. Here we discuss some relevant part of this specification. As we have already mentioned D-Bus provides a system-wide system bus and user-level session bus. Fig. \ref{fig:dbus-daemon} an example of this bus structure. In this paper we have limited our discussion to the latter one and skipped some advance topics like D-Bus security and so on.
%%
\begin{figure}
\begin{center}
\includegraphics[width=7cm,height=3.1cm]{./dia-files/dbus-daemon} % The printed column width is 8.4 cm.
\caption{A typical view of D-Bus message bus system } 
\label{fig:dbus-daemon}
\end{center}
\end{figure}
%%
Here a few basic D-Bus terminologies have been introduced from D-Bus literature.\\
\textbf{D-Bus Connection: }
\textit{DBusConnection} is the structure that a program first uses to initiate talking to the D-Bus daemon, Programs can either use DBUS\_BUS\_SYSTEM or DBUS\_BUS\_SESSION to talk to the respective daemons.\\
\textbf{DBus Message: }
It is simply a message between two process. All the DBus intercommunication are done using \textit{DBusMessage}. These messages can have the following four types: method calls, method returns, signals, and errors. The DBusMessage structure can carry data payload, by appending boolean integers, real numbers, string, arrays, etc. to the message body.\\ 
\textbf{D-Bus Path: }
This is the path of a remote \textit{Object} (hereafter, this is captitalized to avoid ambiguity) of target process, e.g. \/org\/freedesktop\/DBus.\\
\textbf{D-Bus Interface: }
This is the interface on a given Object to talk with, e.g. org.freedesktop.DBus.\\
\textbf{D-Bus Method Call: }
This is a type of DBus message that used to invoke a method on a remote Object.\\
\textbf{D-Bus Signal: }
This is a type of DBus message to make a signal emission. As stated in D-Bus specification, Signal messages must have three header fields: PATH giving the object the signal was emitted from, plus INTERFACE and MEMBER giving the fully-qualified name of the signal. The INTERFACE header is required for signals, though it is optional for method calls. The structure of a signal is shown Fig. \ref{fig:dbus-signal-protocol} and it shows the design of robot-status signal that emits over specified interfaces and paths with a data payload of an integer and a string containing robot-status message.\\
\textbf{D-Bus Error: }
This is the structure that holds the error code which occurs by calling a DBus method.
%%
\begin{figure}
\begin{center}
\includegraphics[width=5cm,height=4cm]{./dia-files/dbus-signal-protocol} % The printed column width is 8.4 cm.
\caption{A typical structure of a D-Bus signal message} 
\label{fig:dbus-signal-protocol}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%
\subsection*{Strategies for Application Integration}
Under D-Bus, there are two basic mechanisms for applications to interact with each other: by calling a remote Object of target application and by emitting a signal for interested applications. To perform a method call on a D-BUS Object, a method call message must be sent to that Object. It will do some processing and return either a method return message or an error message. Signals are different in that they cannot return anything: there is neither a "signal return" message, nor any other type of error message see this \footnote{http://www.ibm.com/developerworks/linux/library/l-dbus.html} for some example use-cases. Thus on D-Bus everything can be done asynchronously without the need of polling.\\ 
D-Bus provides several language bindings for integrating D-Bus to any native application. The core D-BUS API, written in C, is rather low-level and large. On top of this API, bindings integrate with programming languages and environments, including Glib, Python, Qt and Mono. On top of providing language wrappers, the bindings provide environment-specific features. For example, the Glib bindings treat D-BUS connections as GObjects and allow messaging to integrate into the Glib mainloop. The preferred use of D-BUS is definitely using language and environment-specific bindings, both for ease of use and improved functionality (\cite{Love2005}).
%%%%%%%%%%%%%%%%%%%%%
\subsection{BTCom/Myro: E-puck robot control programs}
\label{expt-tools:btcom}
%\subsection{E-puck base library} 
%\subsection{Bluetooth communication (BTCom) interface}
%\subsection{Myro: Python wrapper for E-puck library}
%\subsection{E-puck navigation and obstacle avoidance}
%\subsection{E-puck battery low voltage detection}

E-puck robot comes with a set of software tools and libraries to program and to monitor low-level sensor and actuator values. The low-level C library with driver code of Epuck robot can be downloaded from E-puck website \footnote{www.e-puck.org}. In order to modify and recompile this library E-puck developers recommend both Windows and Linux cross-compling tool-chains. Under Windows, the MPLAB  environment from Microchip \footnote{http://www.microchip.com} can be used with their C30 compiler. We have used this  commercial tool for programming E-puck robot (dsPIC micro-controller), since the Linux counterpart, piklab \footnote{http://piklab.sourceforge.net/} has been found unstable and still under-development. A wide variety of boot-loaders, both under Windows and Linux,  can be used to upload the {\em .hex} firmware files to E-puck robot over Bluetooth. We have found a most reliable E-puck boot-loader  built-in with the trial version  of Webots \footnote{http://www.cyberbotics.com/products/webots/} simulator.  E-puck website also provides various other tools, e.g. Player robot control framework driver \footnote{http://code.google.com/p/epuck-player-driver/} Matlab interfacing program, Epuck-monitor (under Windows) etc. for monitoring (or setting) sensors (or actuator) values.  The default firmware running in E-puck robot is {\em BTCom}.  It initializes  Epuck robot  hardware and waits for user command over Bluetooth serial port. A set of well-defined BTCom commands can be found in Epuck-library documentation. 

For high-level control of E-puck robot, we have used Myro robot-control framework \footnote{http://wiki.roboteducation.org/} that is developed by Institute for Personal Robots in Education. While BTCom provides a set of user commands for controlling the robot it does not take care the setting up the Bluetooth serial connection. Moreover the supplied user commands are very primitive in nature. For example, from a high-level perspectives it is more desirable to command a robot for moving at a specific speed for a given time. Under BTCom, a user can not achieve this without interactively giving low-level motor commands, e.g. set left/right motor speed.  On the other hand, by setting up Myro framework, the Bluetooth communication with host PC and E-puck robot can easily be set-up under Python's pySerial \footnote{http://pyserial.sourceforge.net/} module. This provides an elegant solution for controlling E-puck from software code. Besides, Myro provides a thin wrapper code for E-puck's BTCom for defining high-level user commands. For example, instead of setting left/right motor speed a user can send a forward command with  speed and time-out as its parameters. With the simplicity and interactivity of Python programming, this wrapper makes E-puck programming and debugging very simple and easy. 

The default BTCom has another limitation that it's detection of low battery voltage is almost unnoticeable by naked eye. For running a long time experiment this is critical since we would like to continue our experiments even if a few robots' batteries run out. By the default code of BTCom, a tiny red LED, located near the poer LED in E-puck body, turns on when battery voltage becomes low. This LED light is not visible from a crowd of robots. In order to overcome this issue we modified BTCom so that it can turn of all LEDs when battery voltage becomes critical. The exploited the hardware interrupt signal from Low-Voltage-Detection (LVD) module of E-puck hardware.

Finally we developed our custom navigation and obstacle avoidance algorithms in Python. The navigation function is based on our camera pose information. In each time-step,  robot gets it current pose information from our multi-robot tracking system. It then determines its current coordinate (location) relative to the target object and calculates the differences in pose and orientation. To advance forward, it at first corrects its heading based on the difference and then moves forward for a small fixed distance towards the target. Of course, in every time-step, it also checks that if it is located within the target object's boundary and if that is the case it ceases its motion. Obstacle avoidance algorithm works under the navigation code. While a robot tries to move forward if an obstacle is sensed by its IR sensor it makes a random turn and tries to avoid it. Due to the noisy sensor values, it takes two or a few time-steps to completely get rid of that obstacle. 
%%%%%%%%%%%%%%%%%%%%%
\subsection{BlueZ: Linux's Bluetooth communication stack}
\label{expt-tools:bluez}
%\subsection{Bluez}
%\subsection{Link configuration and management}
The physical communication between the host PC and E-puck robot occurs over Bluetooth wireless radio communication channel.  As defined by the Bluetooth Special Interest Group's official technology  info site \footnote{www.bluetooth.com}
 \begin{quote}
{\em Bluetooth} technology is a wireless communications technology intended to replace the cables connecting portable and/or fixed devices while maintaining high levels of security. The key features of Bluetooth technology are robustness, low power, and low cost.
 \end{quote}
The obvious reason for selecting Bluetooth as the communication technology of E-puck is perhaps   due to it's low cost, low battery usage and universality of hardware and software. Each E-puck robot has a Bluetooth radio link to connect to a host PC or nearby other E-puck robots. Under the hood,  this Bluetooth  chip, LMX9820A \footnote{http://www.national.com/opf/LM/LMX9820A.html},   is  interfaced with a UART (Universal Asynchronous Receiver/Transmitter)  microchip. The bluetooth chip can be used to access to the UART "transparently" using a bluetooth rfcomm channel. Using this mode, one can access the e-puck as if it is connected to a serial port.  According to the specification of LMX9820A, it supports Bluetooth version 1.1 qualification that means the maximum supported data transfer speed is 1Mbit/s. But typically it is configured to use a serial port's 115200 bits/s speed. 

Before starting to use an E-puck robot  one needs to set-up the Bluetooth connection with the robot. Typical Bluetooth connection set-up from a Bluetooth-enabled host PC includes a few manual steps:  detecting the remote Bluetooth device, securely bonding the device (e.g.  exchanging secret keys) and setting-up the target {\em rfcomm} or serial connection (over radio) channel. Various Bluetooth software stacks are available under different OSes. Under Linux, BlueZ \footnote{www.bluez.org} becomes the {\em de-facto} standard software platform. The BlueZ stack was initially developed by Max Krasnyansky at Qualcomm \footnote{http://www.qualcomm.com/} and in 2001  they decided to release it under the GPL. The BlueZ kernel modules, libraries and utilities are known to be working prefect on many architectures supported by Linux. It offers full support for Bluetooth device scanning, securely pairing with devices, automatic rfcomm or serial link configuration, monitoring and so forth. Initial  scanning and secure bonding of E-puck devices can be done by a set of BlueZ tools, namely, {\em hcitool, l2ping, hciconfig, rfcomm} etc. While initializing, BlueZ's  core daemon, {\em bluetoothd}, reads the necessary configuration files (e.g. rfcomm.conf) and dynamically sets up or binds all Bluetooth devices' links and thereafter, routes all low-level communications to them. BlueZ's supplementary package Hcidump offers logging raw data of all Bluetooth communications over a host PC's Bluetooth adapter.  In our host PC, we have used USB dongle type  Bluetooth adapter. We have also used various Linux serial connectivity tools, e.g {\em minicom, picocom etc.} to test link configurations and to send BTCom commands to E-puck robots.

From the above points, we can see that setting up and maintaining connectivity to E-puck robots through Bluetooth links is  not a trivial task. Thus, one needs to consider automating the process of Bluetooth link set-up and verification in order to save time in initializing real experiments. Moreover, comparing with other common wireless technologies, e.g. Wifi, Bluetooth is a relatively low-bandwidth technology. In case of almost all wireless technologies, presence of lots of wireless devices  causes significant noises and interferences. Thus, one also needs to consider the channel capacity or total available bandwidth for communications. Within the context of our experiments, we have got an interesting open question: {\em What is the maximum number of E-puck robots that can talk to host PC simultaneously}. However, finding the answer of this question is beyond the scope of this thesis and here we would only like  to stick with the feasible configuration of Bluetooth links without modifying any low-level protocols or technical implementation.

%\subsection{Bluez}
%\subsection{Link configuration and management}
%%%%%%%%%%%%%%%%%%%%%
\subsection{Python's Multiprocessing: process-based multi-threading}
\label{expt-tools:python}
The real-time interactions among multiple software applications often require concurrency and synchronization, to some degrees, in their functions. Although a common inter-process communication (IPC) protocol, e.g. D-Bus, solves the problem of data-sharing among different application processes, synchronization of data in various processes remains a challenging issue.   The idea of simultaneous and parallel execution of different part of application codes without any IPC, typically on  multiple CPU cores, introduces the notion of multi-threading programming. Both process-based and thread-based approach of  program execution has pros and cons. Threads are light weight and they can share memory and state with the parent process without dealing with the complexity of IPC.   Threads can be useful to the algorithms which rely on shared data/state. They can increase throughput by processing more information faster. They can also reduce latency and improve the responsiveness of an application, such as GUI actions. However, since threads  implicitly ``share everything'' – programmers have to protect (lock) anything which will be
shared between threads. Thus thread-based programs are subject to face race conditions or deadlocks among multiple threads.\\
On the other hand, processes are independent process-of-control and they are isolated from each other by the OS. In order to do any data/state sharing they must use some form of IPC to communicate/coordinate. Comparing with threads, processes are big and heavy since process creation takes time and these processes also tends to be large in program size and memory footprint.  Since processes ``share nothing'' -- programmers must explicitly
 share any data/state with suitable mechanism. From a high-level robotic programmer's point of view, both thread-based and process-based application design approaches are disadvantageous. Since thread-based approach requires careful attention in data-sharing it becomes very difficult to design bug-free program in short time-scale. On the other hand process-based approach requires to set-up IPC mechanisms and manage them. However, the latter approach is less likely to produce bugs as data sharing is explicit. 

Almost all modern computer OSes and {\em high-level} programming languages, e.g. C/C++, Java etc. offer multi-threading support. However implementation of multi-threading programming involves lots of low-level thread management activities. In this respect {\em very high-level
} programming languages e.g. Python, Ruby etc. offer more efficient and elegant solutions for dealing with multi-theaded programs. Along with this multi-threading issue, various other factors influence us to use Python for coding our high level robotic programs. For example, Python programs are  {\em interpreted} by various Python interpreters. Unlike dealing with compiling issues in most of the high-level programming languages, Python allows programmers to focus on their  algorithms more quickly and integrate their systems more effectively.  We have found Python's interactive program development process more productive and flexible than non-interactive programming approach found in some other languages. 

Starting from from version 2.6 Python offers an integration of thread-based programming with process-based programming through it Multiprocessing module \footnote{http://docs.python.org/library/multiprocessing.html}. Traditionally, Python offers threads that are real, OS/Kernel level POSIX {\em pthreads}.  But, older Python programs can have only a single thread to be executing within
 the interpreter at once. This restriction is enforced by the
  so-called ``Global Interpreter Lock'' (GIL). This is a lock which must be
 acquired for a thread to enter the interpreter’s space.
 This limits only one thread to  be executing within the Python
 interpreter at once.  This is enforced  in order to keep interpreter maintenance easier.
 But this can also be sidestepped if the application is I/O (e.g. file, socket) bound. A threaded application which makes heavy use of sockets, won’t see
 a a huge GIL penalty.  After  doing a lot of research on various alternatives of this approach, Python community has offered Multiprocessing as a feasible solution to side-step GIL by the CPU-bound applications that  require seamless data/state sharing.   It follows the threading API closely but uses processes and
 IPC under the hood
. It also offers distributed-computing facilities as well, e.g. remote data-sharing and synchronization.  Thus, we have exploited the power and efficiency of Multiprocessing that enables us to make a modular and flexible implementation of multi-robotic software system. Sec \ref{expt-tools:arch} explains some of our implementations of Python Multiprocessing module.\\
Python Multiprocessing offers various mechanisms for sharing data among processes or, more precisely speaking, among sub-processes. We have used a separate {\em Manager} process that handles all the data storage tasks and event-based process synchronizations.  Managers are responsible for network and process-based sharing of data between processes (and machines).
 The primary manager type is the BaseManager - this is the basic Manager object, and can easily be subclassed to share
 data remotely.  We use this Multiprocessing Manager object that runs a server process in one machine and offers data objects through proxies in parallel to many client  processes over network interfaces.
%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multi-robot control architecture}
\label{expt-tools:arch}
Controlling a robot in a well-organized manner involves following a control architecture. As defined by \cite{Mataric2007}, a robot control architecture is a set of guiding principles and constraints for organizing a robot's control system. Since last few decades, robot control architectures has been evolving from deliberative to reactive and hybrid (combination of deliberative and reactive), behaviour-based and to some other forms. It has been well established that hybrid control can bring together the best aspects of both reactive and deliberative control by combining the real-time low-level device control and high-level deliberative action control. Only reactive (or deliberative) control approach is not enough for enabling robots to do complex tasks in a dynamic environments (\cite{Gat1997}).\\
 As shown in Fig. \ref{fig:three-layer-arch}, this is usually achieved by a three-layer architecture composed of deliberator, sequencer and controller . Controller usually works under real-time reactive feedback control loops to do simple tasks by producing primitive robot behaviours, e.g. obstacle avoidance, wall following etc. Deliberator performs time-consuming computations, e.g. running exponential search or computer vision processing algorithms. In order to achieve specific task goals, the middle component, sequencer, typically integrates both deliberator and controller maintaining consistent, robust and timely robot behaviours.\\
%%%
\begin{figure}
\begin{center}
\includegraphics[width=2.5cm,height=2.1cm]{./dia-files/three-layer-arch} % The printed column width is 8.4 cm.
\caption{Classical three layer robot control architecture } 
\label{fig:three-layer-arch}
\end{center}
\end{figure}
%%%%%%%%%%%%%%%%%%
\subsection{Hybrid event-driven architecture on D-Bus ({\em HEAD})}
%%
\begin{figure}
\begin{center}
\includegraphics[width=5.5cm,height=2.1cm]{./dia-files/abstract-arch} % The printed column width is 8.4 cm.
\caption{Our abstract multi-robot control architecture} 
\label{fig:abstract-arch}
\end{center}
\end{figure}
We organized our  multi-robot control architecture, HEAD into three layers as shown in Fig. \ref{fig:abstract-arch}. Although HEAD has been designed by adopting the principles of hybrid architecture it has many distinct features that are absent in a classical hybrid architecture. Firstly, with respect to controller layer, HEAD broadly views sensing and control as communication with external entities. Communication as sensing is not new, e.g. it has been reported in multi-agent learning (\cite{Mataric1998}) and many other places. When robots' on-board computing resources are limited communication can effectively make up their required sensing capabilities. On the other hand, low-level device control is also a series of communication act where actuator commands are typically transmitted over a radio or physical link. Thus, at the bottom layer of HEAD is the communication layer where all external communication takes place over any suitable medium. Components sitting in this layer either act as sensors that can receive environmental state, task information, self pose data etc. via suitable communication link or do the real-time control of devices by sending actuator commands over a target communication channel. \\
Secondly, the apparent tight coupling with sensors to actuators has been reduced by introducing a data and event management (DEM) layer. DEM acts as a short-term storage of sensed data and various events posted by both controller and deliberator components. Task sequencing has been simplified by automated event triggering mechanism. DEM simply creates new event channels and interested components subscribe to this event for reading or writing. If one components updates an event DEM updates subscribed components about this event. Controller and deliberator components synchronize their tasks based on this event signals. DEM efficiently serves newly arrived data to controller and deliberator components by this event mechanism. Thus neither specialized languages are needed to program a sequencer nor cumbersome if/else checks are present in this layer.\\
Finally, deliberator layer of HEAD has been described as an application layer that runs real-application code based on high-level user algorithms as well as low-level sensor data and device states. In classic hybrid architecture the role of this layer has been described mainly in two folds: 1) producing task plan and sending it to sequencer and 2) answering queries made by sequencer. Application layer of HEAD follows the former one by generating plan and queuing it to DEM layer, but it does not support the latter one. DEM layer never makes a query to an application since it acts only as a passive information gateway. Thus this reduced coupling between DEM and application layer has enhanced HEAD with additional robustness and scalability. Additional applications can be added with DEM layer's existing or new event interfaces. Any malfunction or failure in application layer or even in communication layer can be isolated without affecting others. 
%%
\begin{figure*}
\begin{center}
\includegraphics[width=12cm,height=8cm]{./dia-files/concrete-arch} % The printed column width is 8.4 cm.
%\centering
\caption{General outline of {\em HEAD}.}
% Robot-Controller-Client application has been splitted into two parts: one runs locally in server PC and another runs remotely, e.g., in embedded PC 
\label{fig:concrete-arch}
\end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%
\subsection{Robot controller clients}
%%%%
\subsection{D-Bus signal interfaces}
D-Bus IPC allows us to completely decouple the interaction of different parts of software components of HEAD. Here we use the term {\em software component} or application to denote the logical groupings of several sub-processes or threads that works under a mother process or main thread (here we use the term thread and process interchangeably). Software components that follows our three-layer architecture for grouping its processes are called {\em native component} whereas existing software applications are called {\em external component}. As shown in Fig. \ref{fig:concrete-arch}, RCC and TPA are native software components of HEAD whereas SwisTrack, a multi-robot tracking tool (\cite{Lochmatter+2008}) used with HEAD, is called an external component.
\subsection{Software integrations}
In order to integrate both native and external components with HEAD. We have designed two separate communication process: a D-Bus signal reception process, SignalListener, and a D-Bus signal emission process, SignalEmitter. Inside a native component both of this process can communicate with data and event management process, DataManager, by using any suitable mechanisms, such as, multi-threading, multi-processing (offered in Python multiprocessing \footnotetext{http://docs.python.org/library/multiprocessing.html}), TCP or any other networking protocol.\\
Any external component that intend to act as a sensing (actuating) element of HEAD need to implement a SignalEmitter (SignalListener). For example, we extend SwisTrack with D-Bus signal emitting code (aka SignalEmitter) so that it can emit robot pose messages to individual robots D-Bus path under a common interface (uk.ac.newport.SwisTrack). This emitted signal is then caught by SignalListener of individual robot's RCC. Thus the tight-coupling between SwisTrack and RCC has been removed. During run-time SwisTrack can flexibly track variable number of robots and broadcast their corresponding pose messages without any re-compilation of code. Moreover, in worse cases, if SwisTrack or RCC crashes it does not affect any other component at run-time.\\
Expanding SignalEmitter and SignalListener for more D-Bus signals does not require to make any change the IPC implementation code. Let us first look at how to setup a signal emission process.

\textbf{Steps for setting up signal emission:}\\
\textbf{Step 1:} Connect to a D-Bus daemon. Sample C code:
\lstset{language=C,basicstyle=\small}
\begin{lstlisting}
DBusError error;
DBusConnection *conn;
dbus_error_init (&error);
conn = dbus_bus_get (DBUS_BUS_SESSION, &error);
\end{lstlisting}
\textbf{Step 2:} Optionally reserve a D-Bus path or service name (this is not required if the same path is not used by any other process).\\
\textbf{Step 3:} Send signal to a specified path. Sample C code:
\begin{lstlisting} 
DBusMessage *message;
message = dbus_message_new_signal (
``/target/dbus/path",``target.dbus.interface",
``Config");
/* Send the signal */
dbus_connection_send (connection, message, NULL);
dbus_message_unref (message);
\end{lstlisting}
%%
In order to add more signals we just need to repeat step 3 as many times as we need. On the other hand, signal listening can be done by setting up a suitable event loop under any supported language bindings. A basic implementation of both of these processes in Python language can be found in this tutorial \footnote{http://dbus.freedesktop.org/doc/dbus-python/doc/tutorial.html}. 

%%%%%%%%%%%%%%%%%%%%%
%\section{Other software tools and frameworks}
%\subsection{Player/Stage framework}
%\subsection{Webots simulator}
%\subsection{Boost C++ library}
%\subsection{Microchip MPLAB environment}